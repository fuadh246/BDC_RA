{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_and_trim(content, content_type):\n",
    "    if content_type == 'HTML':\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    else:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        try:\n",
    "            tag.attrs = None\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    for linebreak in soup.find_all('br'):\n",
    "        linebreak.extract()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Form type</th>\n",
       "      <th>Form description</th>\n",
       "      <th>Filing date</th>\n",
       "      <th>Reporting date</th>\n",
       "      <th>Act</th>\n",
       "      <th>Film number</th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Filings URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>34</td>\n",
       "      <td>13810352</td>\n",
       "      <td>0001144204-13-026113</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2013-08-08</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>34</td>\n",
       "      <td>131020072</td>\n",
       "      <td>0001144204-13-043799</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-K</td>\n",
       "      <td>Annual report [Section 13 and 15(d), not S-K I...</td>\n",
       "      <td>2013-12-03</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>34</td>\n",
       "      <td>131254591</td>\n",
       "      <td>0001144204-13-065322</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>34</td>\n",
       "      <td>14578102</td>\n",
       "      <td>0001144204-14-006255</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>34</td>\n",
       "      <td>14823110</td>\n",
       "      <td>0001144204-14-028416</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Form type                                   Form description Filing date  \\\n",
       "0      10-Q            Quarterly report [Sections 13 or 15(d)]  2013-05-03   \n",
       "1      10-Q            Quarterly report [Sections 13 or 15(d)]  2013-08-08   \n",
       "2      10-K  Annual report [Section 13 and 15(d), not S-K I...  2013-12-03   \n",
       "3      10-Q            Quarterly report [Sections 13 or 15(d)]  2014-02-06   \n",
       "4      10-Q            Quarterly report [Sections 13 or 15(d)]  2014-05-08   \n",
       "\n",
       "  Reporting date  Act  Film number      Accession number  \\\n",
       "0     2013-03-31   34     13810352  0001144204-13-026113   \n",
       "1     2013-06-30   34    131020072  0001144204-13-043799   \n",
       "2     2013-09-30   34    131254591  0001144204-13-065322   \n",
       "3     2013-12-31   34     14578102  0001144204-14-006255   \n",
       "4     2014-03-31   34     14823110  0001144204-14-028416   \n",
       "\n",
       "                                         Filings URL  \n",
       "0  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "1  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "2  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "3  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "4  https://www.sec.gov/Archives/edgar/data/000147...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'GOLUB CAPITAL BDC, Inc.'\n",
    "}\n",
    "filing_links = pd.read_excel(\n",
    "    \"../GBDC__sec_filing_links.xlsx\", engine='openpyxl')\n",
    "filing_links.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Form type</th>\n",
       "      <th>Form description</th>\n",
       "      <th>Filing date</th>\n",
       "      <th>Reporting date</th>\n",
       "      <th>Act</th>\n",
       "      <th>Film number</th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Filings URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>May 03, 2013</td>\n",
       "      <td>March 31, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>13810352</td>\n",
       "      <td>0001144204-13-026113</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>August 08, 2013</td>\n",
       "      <td>June 30, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>131020072</td>\n",
       "      <td>0001144204-13-043799</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-K</td>\n",
       "      <td>Annual report [Section 13 and 15(d), not S-K I...</td>\n",
       "      <td>December 03, 2013</td>\n",
       "      <td>September 30, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>131254591</td>\n",
       "      <td>0001144204-13-065322</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>February 06, 2014</td>\n",
       "      <td>December 31, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>14578102</td>\n",
       "      <td>0001144204-14-006255</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>May 08, 2014</td>\n",
       "      <td>March 31, 2014</td>\n",
       "      <td>34</td>\n",
       "      <td>14823110</td>\n",
       "      <td>0001144204-14-028416</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Form type                                   Form description  \\\n",
       "0      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "1      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "2      10-K  Annual report [Section 13 and 15(d), not S-K I...   \n",
       "3      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "4      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "\n",
       "         Filing date      Reporting date  Act  Film number  \\\n",
       "0       May 03, 2013      March 31, 2013   34     13810352   \n",
       "1    August 08, 2013       June 30, 2013   34    131020072   \n",
       "2  December 03, 2013  September 30, 2013   34    131254591   \n",
       "3  February 06, 2014   December 31, 2013   34     14578102   \n",
       "4       May 08, 2014      March 31, 2014   34     14823110   \n",
       "\n",
       "       Accession number                                        Filings URL  \n",
       "0  0001144204-13-026113  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "1  0001144204-13-043799  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "2  0001144204-13-065322  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "3  0001144204-14-006255  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "4  0001144204-14-028416  https://www.sec.gov/Archives/edgar/data/000147...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_columns = ['Filing date', 'Reporting date']\n",
    "for col in date_columns:\n",
    "    filing_links[col] = pd.to_datetime(filing_links[col], format='%Y-%m-%d')\n",
    "for col in date_columns:\n",
    "    filing_links[col] = filing_links[col].dt.strftime(\"%B %d, %Y\")\n",
    "filing_links.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filing_links = filing_links.drop(\n",
    "#     filing_links[filing_links['Reporting date'] == 'September 30, 2017'].index)\n",
    "# print(filing_links[filing_links['Reporting date'] == 'September 30, 2017'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/GBDC_Investment.xlsx'\n",
    "# writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content, qtr_date)\n",
    "#     master_table.to_excel(\n",
    "#         writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "#     writer.book.save(path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1476765/000147676517000078/gbdc201710-k.htm'\n",
    "# date = 'September 30, 2017'\n",
    "# url, date\n",
    "# response = requests.get(url, headers=headers)\n",
    "# content = parse_and_trim(response.content, 'HTML')\n",
    "# master_table = extract_tables(content, date)\n",
    "# # process_table_ = process_table(master_table, \"\")\n",
    "# # process_table_.to_excel(\"example.xlsx\")\n",
    "# # process_table_.to_csv('example.csv')\n",
    "# # process_table_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index=-2\n",
    "# url, date = filing_links.iloc[index]['Filings URL'], filing_links.iloc[index]['Reporting date']\n",
    "# url='https://www.sec.gov/Archives/edgar/data/1476765/000162828016021522/gbdc201510-k.htm'\n",
    "# date='September 30, 2016'\n",
    "# print(url, date)\n",
    "# response = requests.get(url, headers=headers)\n",
    "# content = parse_and_trim(response.content, 'HTML')\n",
    "# search_texts = [\n",
    "#     'Consolidated Schedule of Investments',\n",
    "#     'Consolidated Schedule of Investments - (continued)',\n",
    "#         'Consolidated Schedule of Investments (unaudited) - (continued)',\n",
    "#         'Consolidated Schedule of Investments (unaudited)']\n",
    "# all_tags=content.find_all(text=search_texts)\n",
    "# for tag in all_tags:\n",
    "#     print(tag,tag.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     consolidated_schedule_regex = re.compile(r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "#     for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "#         print(\"Tag:\", tag)\n",
    "#         print(\"Next:\", tag.find_next())\n",
    "#         print(\"next:\", tag.next)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content,qtr_date)\n",
    "#     # print(master_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "data_frames = []\n",
    "master_table = None\n",
    "url, date = filing_links.iloc[index]['Filings URL'], filing_links.iloc[index]['Reporting date']\n",
    "print(url, date)\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    print(\"Next:\", tag.next)\n",
    "    table = tag.find_next(\"table\")\n",
    "    if table:\n",
    "        # Extract the table data into a data frame\n",
    "        table_data = []\n",
    "        for row in table.find_all('tr'):\n",
    "            # Include header cells ('th') if necessary\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            row_data = [column.get_text(strip=True) for column in columns]\n",
    "            table_data.append(row_data)\n",
    "\n",
    "        # Create a data frame from the table data and add it to the list\n",
    "        table_df = pd.DataFrame(table_data)\n",
    "        data_frames.append(table_df)\n",
    "\n",
    "        if master_table is None:\n",
    "            master_table = table_df\n",
    "        else:\n",
    "            master_table = pd.concat(\n",
    "                [master_table, table_df], ignore_index=True)\n",
    "\n",
    "    # Print or process the data frames as needed\n",
    "    # for idx, df in enumerate(data_frames):\n",
    "    #     print(f\"Data Frame {idx + 1}:\\n\", df)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                table = tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    # Extract the table data into a data frame\n",
    "                    table_data = []\n",
    "                    for row in table.find_all('tr'):\n",
    "                        # Include header cells ('th') if necessary\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        row_data = [column.get_text(strip=True)\n",
    "                                    for column in columns]\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    # Create a data frame from the table data and add it to the list\n",
    "                    table_df = pd.DataFrame(table_data)\n",
    "                    if len(table_df.columns) > 10:\n",
    "                        data_frames.append(table_df)\n",
    "\n",
    "                        if master_table is None:\n",
    "                            master_table = table_df\n",
    "                        else:\n",
    "                            master_table = pd.concat(\n",
    "                                [master_table, table_df], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url, date)\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "master_table = extract_tables(content, date)\n",
    "print(master_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv(\"test.csv\")\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/2_Test_GBDC_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "    print(html_link, qtr_date)\n",
    "    response = requests.get(html_link, headers=headers)\n",
    "    content = parse_and_trim(response.content, 'HTML')\n",
    "    master_table = extract_tables(content, qtr_date)\n",
    "    master_table.to_excel(\n",
    "        writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "    writer.book .save(path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP 30 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sec.gov/Archives/edgar/data/1476765/000147676517000078/gbdc201710-k.htm\"\n",
    "date = \"September 30, 2017\"\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "contentDUP = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    print(\"Next_date:\", re.search(date_regex_pattern1, tag.find_next().text))\n",
    "    print(\"next:\", tag.next)\n",
    "    print(tag.find_next(\"table\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = extract_tables(content, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### June_30_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                table = tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    # Extract the table data into a data frame\n",
    "                    table_data = []\n",
    "                    for row in table.find_all('tr'):\n",
    "                        # Include header cells ('th') if necessary\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        row_data = [column.get_text(strip=True)\n",
    "                                    for column in columns]\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    # Create a data frame from the table data and add it to the list\n",
    "                    table_df = pd.DataFrame(table_data)\n",
    "                    if len(table_df.columns) > 10:\n",
    "\n",
    "                        if master_table is None:\n",
    "                            master_table = table_df\n",
    "                        else:\n",
    "                            master_table = pd.concat(\n",
    "                                [master_table, table_df], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"June 30, 2016\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "contentDUP = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = extract_tables(content, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv('test.csv')\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Find_next:\", tag.find_next())\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    print(\"Next_date:\", re.search(date_regex_pattern1, tag.find_next().text))\n",
    "    print(\"next:\", tag.next)\n",
    "    print(\"Next next: \", tag.find_next().next.next.next.text)\n",
    "    print(\"next sib: \", tag.find_next_sibling())\n",
    "    # print(tag.find_next(\"table\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "\n",
    "date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    print(\"next:\", tag.next)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    print(\"Next:\", tag.next)\n",
    "    table = tag.find_next(\"table\")\n",
    "    if table:\n",
    "        # Extract the table data into a data frame\n",
    "        table_data = []\n",
    "        for row in table.find_all('tr'):\n",
    "            # Include header cells ('th') if necessary\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            row_data = [column.get_text(strip=True) for column in columns]\n",
    "            table_data.append(row_data)\n",
    "\n",
    "        # Create a data frame from the table data and add it to the list\n",
    "        table_df = pd.DataFrame(table_data)\n",
    "        data_frames.append(table_df)\n",
    "\n",
    "        if master_table is None:\n",
    "            master_table = table_df\n",
    "        else:\n",
    "            master_table = pd.concat(\n",
    "                [master_table, table_df], ignore_index=True)\n",
    "\n",
    "    # Print or process the data frames as needed\n",
    "    # for idx, df in enumerate(data_frames):\n",
    "    #     print(f\"Data Frame {idx + 1}:\\n\", df)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_texts = [\n",
    "    'Consolidated Schedule of Investments',\n",
    "    'Consolidated Schedule of Investments - (continued)',\n",
    "    'Consolidated Schedule of Investments (unaudited) - (continued)',\n",
    "    'Consolidated Schedule of Investments (unaudited)',\n",
    "    'Consolidated Schedule of Investments (unaudited) -\\n(continued)']\n",
    "all_tags = content.find_all(text=search_texts)\n",
    "for tag in all_tags:\n",
    "    print(tag, tag.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", \"w\") as file:\n",
    "    file.write(str(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECHECKING THE EXTRACTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    # master_table = master_table.applymap(\n",
    "    #     lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    # master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "    #     r'^\\s*\\$\\s*$', 0, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    # master_table = master_table.apply(lambda x: x.str.strip().replace(\n",
    "    #     '', np.nan) if x.dtype == \"object\" else x)\n",
    "\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "date = \"December 31, 2013\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "# webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "master_table = extract_tables(content, date)\n",
    "contentDUP = content\n",
    "master_tableDUP = master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv('test_2.csv')\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                table = tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    # Extract the table data into a data frame\n",
    "                    table_data = []\n",
    "                    for row in table.find_all('tr'):\n",
    "                        # Include header cells ('th') if necessary\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        row_data = [column.get_text(strip=True)\n",
    "                                    for column in columns]\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    # Create a data frame from the table data and add it to the list\n",
    "                    table_df = pd.DataFrame(table_data)\n",
    "                    new_table = table_df.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                    new_table = new_table.replace(\n",
    "                        r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                    table_df = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                    if len(table_df.columns) > 10:\n",
    "                        data_frames.append(table_df)\n",
    "\n",
    "                        if master_table is None:\n",
    "                            master_table = table_df\n",
    "                        else:\n",
    "                            master_table = pd.concat(\n",
    "                                [master_table, table_df], ignore_index=True)\n",
    "\n",
    "            # print(master_table)\n",
    "\n",
    "    # master_table = master_table.applymap(\n",
    "    #     lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    # master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "    #     r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "date = \"December 31, 2013\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "# webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "master_table = extract_tables(content, date)\n",
    "contentDUP = content\n",
    "master_tableDUP = master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 31, 2015\n",
      "December 31, 2015 december312015 december312015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_8602/380200694.py:19: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "(505, 27)\n"
     ]
    }
   ],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    try:\n",
    "        if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "            consolidated_schedule_regex = re.compile(\n",
    "                r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "        else:\n",
    "            consolidated_schedule_regex = re.compile(\n",
    "                r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), keep_default_na=False, skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '0').replace('%', '')) if type(x) == str else x)\n",
    "\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "            master_table = master_table.replace('N/A', 'No Value')\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "date = \"December 31, 2015\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "# webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "\n",
    "master_table = extract_tables(content, date)\n",
    "contentDUP = content\n",
    "master_tableDUP = master_table\n",
    "master_table.to_csv('test.csv')\n",
    "with open('text.txt', 'w') as f:\n",
    "    f.write(str(content))\n",
    "\n",
    "filing_links = filing_links[filing_links['Reporting date']\n",
    "                            != 'September 30, 2017']\n",
    "\n",
    "# This does the job\n",
    "# path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/Master_tables_GBDC_Investment.xlsx'\n",
    "# if not os.path.exists('../MT_csv_file'):\n",
    "#     os.makedirs('../MT_csv_file')\n",
    "\n",
    "# writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content, qtr_date)\n",
    "#     master_table.to_excel(\n",
    "#         writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "#     master_table.to_csv(\n",
    "#         '../MT_csv_file/'+qtr_date.replace(',', '')+'.csv')\n",
    "#     writer.book .save(path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
