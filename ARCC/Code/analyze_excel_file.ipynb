{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "filing_links = pd.read_excel(\n",
    "    \"/Users/fuadhassan/Desktop/BDC_RA/ARCC/ARCC__sec_filing_links.xlsx\")\n",
    "\n",
    "path = '/Users/fuadhassan/Desktop/BDC_RA/ARCC/ARCC_Investment.xlsx'\n",
    "xls = pd.ExcelFile(path)\n",
    "all_sheets = pd.read_excel(path, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "# Loop through each sheet and create a DataFrame in the dictionary\n",
    "for sheet_name, sheet_df in all_sheets.items():\n",
    "    dataframes[sheet_name.replace(' ', '_')] = sheet_df\n",
    "for sheet_name, sheet_df in dataframes.items():\n",
    "    print(f\"DataFrame name: {sheet_name} : {sheet_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['Filing date', 'Reporting date']\n",
    "for col in date_columns:\n",
    "    filing_links[col] = pd.to_datetime(filing_links[col], format='%Y-%m-%d')\n",
    "for col in date_columns:\n",
    "    filing_links[col] = filing_links[col].dt.strftime(\"%B %d %Y\")\n",
    "filing_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matches = []  # To store rows of matching data\n",
    "\n",
    "for file, frame in dataframes.items():\n",
    "    if frame.shape[0] < 100:\n",
    "        print(file.replace('_', ' '))\n",
    "        matching_rows = filing_links[filing_links['Reporting date'].str.replace(\n",
    "            ' ', '') == file.replace('_', '')]\n",
    "        if not matching_rows.empty:\n",
    "            matches.append(matching_rows)\n",
    "            print(matching_rows['Filings URL'].values[0])\n",
    "            # webbrowser.open(matching_rows['Filings URL'].values[0])\n",
    "working_class = pd.concat(matches, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_trim(content, content_type):\n",
    "    if content_type == 'HTML':\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    else:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        try:\n",
    "            tag.attrs = None\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    for linebreak in soup.find_all('br'):\n",
    "        linebreak.extract()\n",
    "    for linebreak in soup.find_all('hr'):\n",
    "        linebreak.extract()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775023000036/arcc-20230630.htm'\n",
    "# date = 'June 30, 2023'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775019000013/arccq1-1910q.htm'\n",
    "# date = 'March 31, 2019'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000104746911001575/a2202281z10-k.htm'\n",
    "# # date = 'December 31, 2010'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000110465910055721/a10-17362_110q.htm'\n",
    "# date = 'September 30, 2010'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775018000015/arccq1-1810q.htm'\n",
    "# date = 'March 31, 2018'\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/1287750/000110465913080832/a13-19678_110q.htm'\n",
    "date = 'September 30, 2013'\n",
    "\n",
    "qtr_date = date\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "\n",
    "    master_table = None\n",
    "    all_tags = soup_content.find_all(True)\n",
    "    count = 0\n",
    "\n",
    "    for tag in soup_content.find_all(text=consolidated_schedule_regex):\n",
    "        # print(len(soup_content.find_all(text=consolidated_schedule_regex)))\n",
    "        next_line_text = tag.next.text.strip()\n",
    "        date_str = re.search(date_regex_pattern, next_line_text)\n",
    "\n",
    "        if date_str is None:\n",
    "            next_line_text = tag.find_next(\n",
    "                text=re.compile(date_regex_pattern)).text\n",
    "            date_str = re.search(date_regex_pattern, next_line_text)\n",
    "\n",
    "        if date_str is None:\n",
    "            next_line = tag.next.text.strip()\n",
    "            date_str = re.search(date_regex_pattern, next_line)\n",
    "\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "\n",
    "            if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                count += 1\n",
    "                html_table = tag.find_next('table')\n",
    "                while html_table:\n",
    "                    new_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                    new_table = new_table.replace(\n",
    "                        r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                    new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                    if master_table is None:\n",
    "                        master_table = new_table\n",
    "                    else:\n",
    "                        master_table = pd.concat(\n",
    "                            [master_table, new_table], ignore_index=True)\n",
    "                    print(\"ran\")\n",
    "\n",
    "                    # if html_table.find(text=re.compile(r'Other than', re.IGNORECASE)):\n",
    "                    #     print(html_table.find(text=re.compile(\n",
    "                    #         r'Other than', re.IGNORECASE)))\n",
    "                    #     break\n",
    "                    # elif html_table.find_next(text='Derivative Instruments'):\n",
    "                    #     print(html_table.find_next(\n",
    "                    #         text='Derivative Instruments'))\n",
    "                    #     break\n",
    "                    # elif html_table.find_next(text=consolidated_schedule_regex):\n",
    "                    #     next_line_text = tag.find_next(\n",
    "                    #         text=re.compile(date_regex_pattern)).text\n",
    "                    #     date_str2 = re.search(\n",
    "                    #         date_regex_pattern, next_line_text)\n",
    "                    #     if date_str2.group(1) == date_str:\n",
    "                    #         break\n",
    "                    print(html_table.find_previous_sibling())\n",
    "                    html_table = html_table.find_next('table')\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "master_table = extract_tables(content, qtr_date)\n",
    "master_table.to_csv('example.csv')\n",
    "master_table.to_excel('example.xlsx')\n",
    "\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775023000036/arcc-20230630.htm'\n",
    "# date = 'June 30, 2023'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775019000013/arccq1-1910q.htm'\n",
    "# date = 'March 31, 2019'\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/1287750/000104746914001349/a2218339z10-k.htm'\n",
    "date = 'December 31, 2013'\n",
    "\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775018000015/arccq1-1810q.htm'\n",
    "# date = 'March 31, 2018'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000110465913080832/a13-19678_110q.htm'\n",
    "# date = 'September 30, 2013'\n",
    "\n",
    "qtr_date = date\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "\n",
    "    master_table = None\n",
    "    all_tags = soup_content.find_all(True)\n",
    "    count = 0\n",
    "\n",
    "    for tag in soup_content.find_all(True):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if re.fullmatch(date_regex_pattern2, text):\n",
    "            date_str = re.search(date_regex_pattern1, text)\n",
    "            print(tag)\n",
    "            print(date_str)\n",
    "            if date_str is not None:\n",
    "                date_str = str(date_str.group(1))\n",
    "                date_str = unicodedata.normalize('NFKD', date_str)\n",
    "\n",
    "            if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                html_table = tag.find_next('table')\n",
    "                print('Table found: ')\n",
    "                html_table = tag.find_next('table')\n",
    "                if master_table is None:\n",
    "                    master_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    master_table = master_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                              regex=True)\n",
    "                    master_table = master_table.dropna(how='all', axis=0)\n",
    "                else:\n",
    "                    new_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                    new_table = new_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                        regex=True)\n",
    "                    new_table = new_table.dropna(how='all', axis=0)\n",
    "                    # print('head')\n",
    "                    # print(new_table.head()) # text\n",
    "                    master_table = master_table.append(\n",
    "                        new_table.dropna(how='all', axis=0).reset_index(\n",
    "                            drop=True).drop(index=0),\n",
    "                        ignore_index=True)\n",
    "\n",
    "    return master_table\n",
    "\n",
    "\n",
    "master_table = extract_tables(content, qtr_date)\n",
    "# master_table.to_csv('example.csv')\n",
    "# master_table.to_excel('example.xlsx')\n",
    "\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one works for all other\n",
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775023000036/arcc-20230630.htm'\n",
    "# date = 'June 30, 2023'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775019000013/arccq1-1910q.htm'\n",
    "# date = 'March 31, 2019'\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/1287750/000104746914001349/a2218339z10-k.htm'\n",
    "date = 'December 31, 2013'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775018000015/arccq1-1810q.htm'\n",
    "# date = 'March 31, 2018'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000110465913080832/a13-19678_110q.htm'\n",
    "# date = 'September 30, 2013'\n",
    "\n",
    "qtr_date = date\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    master_table = None\n",
    "\n",
    "    for tag in soup_content.find_all(True):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if re.fullmatch(date_regex_pattern2, text):\n",
    "            date_str = re.search(date_regex_pattern1, text)\n",
    "            next_line = tag.find_next().text\n",
    "            if re.search('dollar', next_line):\n",
    "                print(tag)\n",
    "                print(True)\n",
    "                print(date_str.group(1))\n",
    "                if date_str is not None:\n",
    "                    date_str = str(date_str.group(1))\n",
    "                    date_str = unicodedata.normalize('NFKD', date_str)\n",
    "                    # print(date_str)\n",
    "                if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                    # print('match')\n",
    "                    html_table = tag.find_next('table')\n",
    "                    print('Table found: ')\n",
    "                    html_table = tag.find_next('table')\n",
    "                    if master_table is None:\n",
    "                        master_table = pd.read_html(\n",
    "                            html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                        master_table = master_table.applymap(lambda x: unicodedata.normalize(\n",
    "                            'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                        master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                                  regex=True)\n",
    "                        master_table = master_table.dropna(how='all', axis=0)\n",
    "                    else:\n",
    "                        new_table = pd.read_html(\n",
    "                            html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                        new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                            'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                        new_table = new_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                            regex=True)\n",
    "                        new_table = new_table.dropna(how='all', axis=0)\n",
    "                        # print('head')\n",
    "                        # print(new_table.head()) # text\n",
    "                        master_table = master_table.append(\n",
    "                            new_table.dropna(how='all', axis=0).reset_index(\n",
    "                                drop=True).drop(index=0),\n",
    "                            ignore_index=True)\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "master_table = extract_tables(content, qtr_date)\n",
    "master_table.to_csv('example.csv')\n",
    "master_table.to_excel('example.xlsx')\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775023000036/arcc-20230630.htm'\n",
    "# date = 'June 30, 2023'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775019000013/arccq1-1910q.htm'\n",
    "# date = 'March 31, 2019'\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/1287750/000104746914001349/a2218339z10-k.htm'\n",
    "date = 'December 31, 2013'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775018000015/arccq1-1810q.htm'\n",
    "# date = 'March 31, 2018'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000110465913080832/a13-19678_110q.htm'\n",
    "# date = 'September 30, 2013'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775018000007/arccq4-1710k.htm'\n",
    "# date = 'December 31, 2017'\n",
    "\n",
    "qtr_date = date\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    master_table = None\n",
    "\n",
    "    for tag in soup_content.find_all(text=re.compile(date_regex_pattern2)):\n",
    "\n",
    "        date_str = re.search(date_regex_pattern1, tag.text)\n",
    "        find_next = tag.find_next().text\n",
    "        next_line = tag.next.text\n",
    "\n",
    "        if re.search('dollar amounts', find_next) or re.search('dollar amounts', next_line):\n",
    "            print(date_str.group(1))\n",
    "            if date_str is not None:\n",
    "                date_str = str(date_str.group(1))\n",
    "                date_str = unicodedata.normalize('NFKD', date_str)\n",
    "                # print(date_str)\n",
    "            if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                # print(tag)\n",
    "                # print(find_next)\n",
    "                # print(next_line)\n",
    "                # print('match')\n",
    "                html_table = tag.find_next('table')\n",
    "                print('Table found: ')\n",
    "                html_table = tag.find_next('table')\n",
    "                if master_table is None:\n",
    "                    master_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    master_table = master_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                              regex=True)\n",
    "                    master_table = master_table.dropna(how='all', axis=0)\n",
    "                else:\n",
    "                    new_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                    new_table = new_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                        regex=True)\n",
    "                    new_table = new_table.dropna(how='all', axis=0)\n",
    "                    # print('head')\n",
    "                    # print(new_table.head()) # text\n",
    "                    master_table = master_table.append(\n",
    "                        new_table.dropna(how='all', axis=0).reset_index(\n",
    "                            drop=True).drop(index=0),\n",
    "                        ignore_index=True)\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "master_table = extract_tables(content, qtr_date)\n",
    "master_table.to_csv('example.csv')\n",
    "master_table.to_excel('example.xlsx')\n",
    "# master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(soi_table_df, append_str):\n",
    "    soi_table_df = soi_table_df.replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "    soi_table_df = soi_table_df.dropna(how='all', axis=1)\n",
    "    soi_table_df = soi_table_df.dropna(\n",
    "        how='all', axis=0).reset_index(drop=True)\n",
    "    # print('1: ' + str(soi_table_df.shape))\n",
    "\n",
    "    # Separate header and data\n",
    "    soi_table_header = soi_table_df.iloc[0].dropna(how='any')\n",
    "    soi_table_data_df = soi_table_df.rename(\n",
    "        columns=soi_table_df.iloc[0]).drop(soi_table_df.index[0])\n",
    "    # print('2: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # drops all the rows that contains header\n",
    "    soi_table_data_df = soi_table_data_df[soi_table_data_df[soi_table_data_df.columns[0]]\n",
    "                                          != soi_table_data_df.columns[0]]\n",
    "    # print('3: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop rows with only two non-null values becuase all the subtotal contain 2 value only\n",
    "    soi_table_data_df = soi_table_data_df.dropna(thresh=3)\n",
    "    # print('4: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # replace all the - in the data table with 0\n",
    "    soi_table_data_df = soi_table_data_df.replace('-', 0, regex=False)\n",
    "\n",
    "    # fix the all the nan value column , Amortized Cost, Fair Value\n",
    "    columns_to_fill = ['Amortized Cost', 'Fair Value']\n",
    "    for col in columns_to_fill:\n",
    "        col_index = soi_table_data_df.columns.str.replace(\n",
    "            ' ', '').get_loc(col.replace(' ', ''))\n",
    "        next_col_index = col_index + 1\n",
    "        for i in range(len(soi_table_data_df)):\n",
    "            current_value = soi_table_data_df.iat[i, col_index]\n",
    "            if pd.isna(current_value) and next_col_index < len(soi_table_data_df.columns):\n",
    "                next_valid_index = next((j for j, v in enumerate(\n",
    "                    soi_table_data_df.iloc[i, next_col_index:], start=next_col_index) if pd.notna(v)), None)\n",
    "\n",
    "                if next_valid_index is not None:\n",
    "                    next_value = soi_table_data_df.iat[i, next_valid_index]\n",
    "                    soi_table_data_df.iat[i, col_index] = next_value\n",
    "                    soi_table_data_df.iat[i, next_valid_index] = pd.NA\n",
    "\n",
    "    # drops everything after FairValue\n",
    "    if 'FairValue' in soi_table_data_df.columns.str.replace(' ', ''):\n",
    "        start_index = soi_table_data_df.columns.str.replace(\n",
    "            ' ', '').get_loc('FairValue')\n",
    "        soi_table_data_df = soi_table_data_df.iloc[:, :start_index+1]\n",
    "\n",
    "    # Drop rows with only two non-null values this one recheacks\n",
    "    soi_table_data_df = soi_table_data_df.dropna(thresh=3)\n",
    "\n",
    "    # Forward fill the first two columns\n",
    "    col_indices = [0, 1]\n",
    "    soi_table_data_df.iloc[:, col_indices] = soi_table_data_df.iloc[:, col_indices].fillna(\n",
    "        method='ffill')\n",
    "    # print('7: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop rows with all missing values\n",
    "    soi_table_df = soi_table_df.dropna(how='all')\n",
    "    # print('5: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop columns with all missing values\n",
    "    soi_table_data_df = soi_table_data_df.dropna(how='all', axis=1)\n",
    "    # print('6: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # fill all the nan with 0\n",
    "    soi_table_data_df = soi_table_data_df.fillna(0)\n",
    "\n",
    "    # converts to number\n",
    "    # cols_to_convert = ['Shares/Units', 'Principal',\n",
    "    #                    'Amortized Cost', 'Fair Value']\n",
    "    # for col in cols_to_convert:\n",
    "    #     if col.replace(' ', '') in soi_table_data_df.columns.str.replace(' ', ''):\n",
    "    #         col_index = soi_table_data_df.columns.str.replace(\n",
    "    #             ' ', '').get_loc(col.replace(' ', ''))\n",
    "    #         soi_table_data_df.iloc[:, col_index] = pd.to_numeric(\n",
    "    #             soi_table_data_df.iloc[:, col_index], errors='coerce').fillna(0)\n",
    "\n",
    "    cols_to_convert = ['Shares/Units', 'Principal',\n",
    "                       'Amortized Cost', 'Fair Value']\n",
    "    for col in cols_to_convert:\n",
    "        if col.replace(' ', '') in soi_table_data_df.columns.str.replace(' ', ''):\n",
    "            col_index = soi_table_data_df.columns.str.replace(\n",
    "                ' ', '').get_loc(col.replace(' ', ''))\n",
    "            converted_data = pd.to_numeric(\n",
    "                soi_table_data_df.iloc[:, col_index], errors='coerce').fillna(0)\n",
    "            soi_table_data_df[soi_table_data_df.columns[col_index]\n",
    "                              ] = converted_data\n",
    "\n",
    "    soi_table_data_df = soi_table_data_df.reset_index(drop=True)\n",
    "\n",
    "    # keeping the total for now to check if the total is right\n",
    "    # Drop rows based on regex pattern (e.g., 'subtotal' or 'total')\n",
    "    # pattern = r'^([Ss]ubtotal)|([Tt]otal)'\n",
    "    # mask = soi_table_data_df.apply(lambda row: row.astype(\n",
    "    #     str).str.contains(pattern, case=False, na=False)).any(axis=1)\n",
    "    # soi_table_data_df = soi_table_data_df[~mask]\n",
    "    # # print('4: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # we dont need this here because we are dropping subtotal before\n",
    "    # Drop rows labeled as subtotals\n",
    "    # subtotal_rows = soi_table_data_df[soi_table_data_df['Company (1)'].str.replace(' ', '').str.contains(\n",
    "    #     'subtotal', case=False, na=False)]\n",
    "    # soi_table_data_df = soi_table_data_df[~soi_table_data_df.index.isin(\n",
    "    #     subtotal_rows.index)]\n",
    "    # print('3: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # we dont need this here because we are dropping subtotal before\n",
    "    # columns_to_drop = []\n",
    "    # for column in soi_table_data_df.columns:\n",
    "    #     # Check for NaN values in the column\n",
    "    #     # Use .item() to get a single boolean value\n",
    "    #     if soi_table_data_df[column].isna().any().item():\n",
    "    #         columns_to_drop.append(column)\n",
    "    # soi_table_data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    # print('Final: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    return soi_table_data_df\n",
    "\n",
    "\n",
    "process_table_ = process_table(master_table, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775023000036/arcc-20230630.htm'\n",
    "# date = 'June 30, 2023'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775019000013/arccq1-1910q.htm'\n",
    "# date = 'March 31, 2019'\n",
    "url = 'https://www.sec.gov/Archives/edgar/data/1287750/000104746914001349/a2218339z10-k.htm'\n",
    "date = 'December 31, 2013'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775018000015/arccq1-1810q.htm'\n",
    "# date = 'March 31, 2018'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000110465913080832/a13-19678_110q.htm'\n",
    "# date = 'September 30, 2013'\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1287750/000128775018000007/arccq4-1710k.htm'\n",
    "# date = 'December 31, 2017'\n",
    "\n",
    "qtr_date = date\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    master_table = None\n",
    "\n",
    "    for tag in soup_content.find_all(text=re.compile(date_regex_pattern2)):\n",
    "\n",
    "        date_str = re.search(date_regex_pattern1, tag.text)\n",
    "        find_next = tag.find_next().text\n",
    "        next_line = tag.next.text\n",
    "        if re.search('dollar amounts', find_next) or re.search('dollar amounts', next_line):\n",
    "            print(date_str.group(1))\n",
    "            if date_str is not None:\n",
    "                date_str = str(date_str.group(1))\n",
    "                date_str = unicodedata.normalize('NFKD', date_str)\n",
    "                # print(date_str)\n",
    "            if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                # print(tag)\n",
    "                # print(find_next)\n",
    "                # print(next_line)\n",
    "                # print('match')\n",
    "                html_table = tag.find_next('table')\n",
    "                print('Table found: ')\n",
    "                # while soup_content.find_next(text=re.compile(date_regex_pattern2)) is not False:\n",
    "                if master_table is None:\n",
    "                    master_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    master_table = master_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                              regex=True)\n",
    "                    master_table = master_table.dropna(how='all', axis=0)\n",
    "                else:\n",
    "                    new_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                    new_table = new_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "                                                                                        regex=True)\n",
    "                    new_table = new_table.dropna(how='all', axis=0)\n",
    "                    # print('head')\n",
    "                    # print(new_table.head()) # text\n",
    "                    master_table = master_table.append(\n",
    "                        new_table.dropna(how='all', axis=0).reset_index(\n",
    "                            drop=True).drop(index=0),\n",
    "                        ignore_index=True)\n",
    "\n",
    "                # while soup_content.find_next(text=re.compile(date_regex_pattern2)) is not False:\n",
    "                # while html_table.find_next('table'):\n",
    "                #     print(html_table)\n",
    "                #     if soup_content.find(text=re.compile(date_regex_pattern2)):\n",
    "                #         print('broke')\n",
    "                #         break\n",
    "                    html_table = html_table.find_next('table')\n",
    "\n",
    "    # master_table = master_table.applymap(\n",
    "    #     lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    # master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "    #     r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    # print(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "master_table = extract_tables(content, qtr_date)\n",
    "# master_table.to_csv('example.csv')\n",
    "# master_table.to_excel('example.xlsx')\n",
    "# master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
