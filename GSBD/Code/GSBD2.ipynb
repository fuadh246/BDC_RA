{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from openpyxl import workbook\n",
    "import re\n",
    "import os\n",
    "import webbrowser\n",
    "import unicodedata\n",
    "from Helper_package import Helper\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Goldman Sachs BDC, Inc. GSBD on NYSE'\n",
    "}\n",
    "CIK = '0001572694'\n",
    "\n",
    "filing_data = Helper.fetch_filing_data(cik=CIK, headers=headers)\n",
    "if filing_data is not None:\n",
    "    # Write DataFrame to Excel file with auto-adjusting column widths\n",
    "    file_name = \"../GSBC_sec_filing_links.xlsx\"\n",
    "    with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n",
    "        filing_data.to_excel(writer, index=False)\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        for i, col in enumerate(filing_data.columns):\n",
    "            column_len = max(filing_data[col].astype(\n",
    "                str).str.len().max(), len(col)) + 2\n",
    "            worksheet.set_column(i, i, column_len)\n",
    "\n",
    "    print(f\"Data written to {file_name}\")\n",
    "\n",
    "filing_links = Helper.get_filing_links('../GSBC_sec_filing_links.xlsx')\n",
    "print(\"Filing link recived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_links['reportDate'] = pd.to_datetime(\n",
    "    filing_links['reportDate']).dt.strftime(\"%B %d, %Y\")\n",
    "print(\"Date was converted to '%B %d, %Y' format and back to\")\n",
    "filing_links['reportDate'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_links.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_links_validity(filing_links):\n",
    "    '''\n",
    "        Checks the validity of each file link in the DataFrame.\n",
    "    '''\n",
    "    valid_links = []\n",
    "    invalid_links = []\n",
    "    for index, row in filing_links.iterrows():\n",
    "        link = row['fileLink']\n",
    "        try:\n",
    "            response = requests.head(link, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            valid_links.append(link)\n",
    "        except Exception as e:\n",
    "            invalid_links.append((link, str(e)))\n",
    "    if invalid_links:\n",
    "        print(\"\\nInvalid Links:\")\n",
    "        for link, error_message in invalid_links:\n",
    "            print(f\"{link}: {error_message}\")\n",
    "\n",
    "    if len(valid_links) == filing_links.shape[0]:\n",
    "        print(\"All Valid Links\")\n",
    "\n",
    "\n",
    "check_links_validity(filing_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = filing_links.iloc[18]\n",
    "print(test_file)\n",
    "content = Helper.get_content(Helper.get_response(\n",
    "    url=test_file['fileLink'], headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "\n",
    "\n",
    "def extract_tables(content, report_Date) -> pd.DataFrame:\n",
    "    master_table = pd.DataFrame()\n",
    "    print(f\"Extractiong File: {report_Date}\")\n",
    "    for tag in content.findAll(string=consolidated_schedule_regex):\n",
    "        try:\n",
    "            date_matches = re.findall(date_regex_pattern, tag.text)\n",
    "        except Exception as e:\n",
    "            print(f'Could not find date on extract_tables() : {e}')\n",
    "\n",
    "        if date_matches and len(date_matches) == 1:\n",
    "            table_date = date_matches[0]\n",
    "            if table_date is not None and unicodedata.normalize('NFKD', table_date) == unicodedata.normalize('NFKD', report_Date):\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), na_values=\"No value\", skiprows=0, flavor='bs4')[0]\n",
    "                # new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                #     'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '0').replace('%', '').replace('(', '').replace(')', '')) if type(x) == str else x)\n",
    "\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "\n",
    "    return master_table\n",
    "\n",
    "\n",
    "extract_tables(content=content,\n",
    "               report_Date=test_file['reportDate']).to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/fuadhassan/Desktop/BDC_RA/GSBD/New_Master_tables_GSBD_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for index, (url, reporting_date) in enumerate(zip(filing_links['fileLink'], filing_links['reportDate'])):\n",
    "    try:\n",
    "        content = Helper.get_content(\n",
    "            Helper.get_response(url=url, headers=headers))\n",
    "    except Exception as e:\n",
    "        print(f'failed to get the content: {e}')\n",
    "\n",
    "    master_table = extract_tables(content, reporting_date)\n",
    "    master_table.to_csv(\n",
    "        '../MT_csv_files_2/'+reporting_date.replace(',', '')+'.csv')\n",
    "    master_table.to_excel(\n",
    "        writer, sheet_name=reporting_date.replace(',', ''), index=False)\n",
    "    writer.book .save(path)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
