{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses and trims HTML content by removing all attributes from HTML tags and removing line break tags from the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_trim(content, content_type):\n",
    "    if content_type == 'HTML':\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "    else:\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        try:\n",
    "            tag.attrs = None\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    for linebreak in soup.find_all('br'):\n",
    "        linebreak.extract()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove_multiple_spaces to replace multiple spaces with a single space in a string, and find_qrt_date to extract and format a quarterly date from text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(string):\n",
    "    pattern = r'\\s+'\n",
    "    replaced_string = re.sub(pattern, ' ', string)\n",
    "    return replaced_string\n",
    "\n",
    "\n",
    "def find_qrt_date(content):\n",
    "    qtr_date = content.find_all(text=re.compile(\n",
    "        r'for\\s+(the\\s+)?(fiscal\\s+)?year\\s+ended\\s+|for\\s+the\\s+quarter\\s+ended\\s+|for\\s+the\\s+quarterly\\s+period\\s+ended\\s+', re.IGNORECASE))\n",
    "    qtr_match = re.search(\n",
    "        r'([A-Za-z]+)\\s+(\\d{1,2}),\\s+(\\d{4})', qtr_date[0].replace('\\n', ''))\n",
    "    if qtr_match is None:\n",
    "        qtr_match = qtr_match = re.search(\n",
    "            r'([A-Za-z]+) (\\d{1,2}), (\\d{4})', qtr_date[1])\n",
    "    return remove_multiple_spaces(str(qtr_match.group()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "filing_links = pd.read_excel(\n",
    "    \"../ARCC__sec_filing_links.xlsx\")\n",
    "filing_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops all the amendment filing\n",
    "filing_links = filing_links.drop(filing_links[filing_links['Form description'].str.contains(\n",
    "    'amendment', case=False)].index).reset_index(drop=True)\n",
    "filing_links['Reporting date'] = pd.to_datetime(filing_links['Reporting date'])\n",
    "filing_links = filing_links[filing_links['Reporting date'] >= '2013-03-31']\n",
    "filing_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['Filing date', 'Reporting date']\n",
    "for col in date_columns:\n",
    "    filing_links[col] = pd.to_datetime(filing_links[col], format='%Y-%m-%d')\n",
    "for col in date_columns:\n",
    "    filing_links[col] = filing_links[col].dt.strftime(\"%B %d, %Y\")\n",
    "filing_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_links.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(soup_content, qtr_date):\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    master_table = None\n",
    "    all_tags = soup_content.find_all(True)\n",
    "\n",
    "    for tag in soup_content.find_all(text=re.compile(date_regex_pattern2)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.text)\n",
    "        find_next = tag.find_next().text\n",
    "        next_line = tag.next.text\n",
    "        if re.search('dollar amounts', find_next) or re.search('dollar amounts', next_line):\n",
    "            if date_str is not None:\n",
    "                date_str = str(date_str.group(1))\n",
    "                date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                html_table = tag.find_next('table')\n",
    "\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_manual(soup_content, qtr_date):\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    master_table = None\n",
    "    for tag in soup_content.find_all(text=re.compile(date_regex_pattern2)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.text)\n",
    "        find_next = tag.find_next().text\n",
    "        next_line = tag.next.text\n",
    "        if re.search('dollar amounts', find_next) or re.search('dollar amounts', next_line):\n",
    "            # print(date_str.group(1))\n",
    "            if date_str is not None:\n",
    "                date_str = str(date_str.group(1))\n",
    "                date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                html_table = tag.find_next('table')\n",
    "                while html_table:\n",
    "                    new_table = pd.read_html(\n",
    "                        html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                    new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                    new_table = new_table.replace(\n",
    "                        r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                    new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                    if master_table is None:\n",
    "                        master_table = new_table\n",
    "                    else:\n",
    "                        master_table = pd.concat(\n",
    "                            [master_table, new_table], ignore_index=True)\n",
    "\n",
    "                    if date_str.replace(',', '').strip().lower() in 'December 31, 2013'.replace(',', '').strip().lower() or date_str.replace(',', '').strip().lower() in 'December 31, 2014'.replace(',', '').strip().lower():\n",
    "                        if html_table.find(text=re.compile(r'Food and Beverage', re.IGNORECASE)):\n",
    "                            break\n",
    "                    if date_str.replace(',', '').strip().lower() in 'December 31, 2015'.replace(',', '').strip().lower() or date_str.replace(',', '').strip().lower() in 'December 31, 2016'.replace(',', '').strip().lower():\n",
    "                        if html_table.find(text=re.compile(r'Computers and Electronics', re.IGNORECASE)):\n",
    "                            break\n",
    "                    html_table = html_table.find_next('table')\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    value = master_table.iloc[:, 8].isna() & ~master_table.iloc[:, 9].isna()\n",
    "    master_table.loc[value, [8, 9]] = master_table.loc[value, [9, 8]].values\n",
    "\n",
    "    print(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(soi_table_df, append_str):\n",
    "    soi_table_df = soi_table_df.replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "    soi_table_df = soi_table_df.dropna(how='all', axis=1)\n",
    "    soi_table_df = soi_table_df.dropna(\n",
    "        how='all', axis=0).reset_index(drop=True)\n",
    "    # print('1: ' + str(soi_table_df.shape))\n",
    "\n",
    "    # Separate header and data\n",
    "    soi_table_header = soi_table_df.iloc[0].dropna(how='any')\n",
    "    soi_table_data_df = soi_table_df.rename(\n",
    "        columns=soi_table_df.iloc[0]).drop(soi_table_df.index[0])\n",
    "    # print('2: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # drops all the rows that contains header\n",
    "    soi_table_data_df = soi_table_data_df[soi_table_data_df[soi_table_data_df.columns[0]]\n",
    "                                          != soi_table_data_df.columns[0]]\n",
    "\n",
    "    # print('3: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "# keeps the Industry row\n",
    "    soi_table_data_df['get_Industry'] = None\n",
    "\n",
    "    for index, row in soi_table_data_df.iterrows():\n",
    "        if row.count() == 1:\n",
    "            soi_table_data_df.loc[index+1, 'get_Industry'] = row.iloc[0]\n",
    "            soi_table_data_df = soi_table_data_df.drop(index)\n",
    "\n",
    "    soi_table_data_df.insert(0, 'Industry', soi_table_data_df['get_Industry'])\n",
    "    # Drop rows with only two non-null values becuase all the subtotal contain 2 value only\n",
    "    soi_table_data_df = soi_table_data_df.dropna(thresh=3)\n",
    "    soi_table_data_df['Industry'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # replace all the - in the data table with 0\n",
    "    soi_table_data_df = soi_table_data_df.replace('-', 0, regex=False)\n",
    "\n",
    "    # fix the all the nan value column , Amortized Cost, Fair Value\n",
    "    columns_to_fill = ['Amortized Cost', 'Fair Value']\n",
    "    for col in columns_to_fill:\n",
    "        col_index = soi_table_data_df.columns.str.replace(\n",
    "            ' ', '').get_loc(col.replace(' ', ''))\n",
    "        next_col_index = col_index + 1\n",
    "        for i in range(len(soi_table_data_df)):\n",
    "            current_value = soi_table_data_df.iat[i, col_index]\n",
    "            if pd.isna(current_value) and next_col_index < len(soi_table_data_df.columns):\n",
    "                next_valid_index = next((j for j, v in enumerate(\n",
    "                    soi_table_data_df.iloc[i, next_col_index:], start=next_col_index) if pd.notna(v)), None)\n",
    "\n",
    "                if next_valid_index is not None:\n",
    "                    next_value = soi_table_data_df.iat[i, next_valid_index]\n",
    "                    soi_table_data_df.iat[i, col_index] = next_value\n",
    "                    soi_table_data_df.iat[i, next_valid_index] = pd.NA\n",
    "\n",
    "    # drops everything after FairValue\n",
    "    if 'FairValue' in soi_table_data_df.columns.str.replace(' ', ''):\n",
    "        start_index = soi_table_data_df.columns.str.replace(\n",
    "            ' ', '').get_loc('FairValue')\n",
    "        soi_table_data_df = soi_table_data_df.iloc[:, :start_index+1]\n",
    "\n",
    "    # Drop rows with only two non-null values this one recheacks\n",
    "\n",
    "    # Forward fill the first two columns\n",
    "    col_indices = [0, 1, 2]\n",
    "    soi_table_data_df.iloc[:, col_indices] = soi_table_data_df.iloc[:, col_indices].fillna(\n",
    "        method='ffill')\n",
    "    # print('7: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop rows with all missing values\n",
    "    soi_table_df = soi_table_df.dropna(how='all', axis=1)\n",
    "    # print('5: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop columns with all missing values\n",
    "    soi_table_data_df = soi_table_data_df.dropna(how='all', axis=1)\n",
    "    # print('6: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    cols_to_convert = ['Shares/Units', 'Principal',\n",
    "                       'Amortized Cost', 'Fair Value']\n",
    "    for col in cols_to_convert:\n",
    "        if col.replace(' ', '') in soi_table_data_df.columns.str.replace(' ', ''):\n",
    "            col_index = soi_table_data_df.columns.str.replace(\n",
    "                ' ', '').get_loc(col.replace(' ', ''))\n",
    "            converted_data = pd.to_numeric(\n",
    "                soi_table_data_df.iloc[:, col_index], errors='coerce').fillna(0)\n",
    "            soi_table_data_df[soi_table_data_df.columns[col_index]\n",
    "                              ] = converted_data\n",
    "\n",
    "    soi_table_data_df = soi_table_data_df.reset_index(drop=True)\n",
    "\n",
    "    # print('Final: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    return soi_table_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "manual = ['December 31, 2013', 'December 31, 2014',\n",
    "          'December 31, 2015', 'December 31, 2016']\n",
    "path = '../ARCC_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "    print(qtr_date)\n",
    "    print('Starting file # ', count)\n",
    "    response = requests.get(html_link, headers=headers)\n",
    "    content = parse_and_trim(response.content, 'HTML')\n",
    "    print('Getting content done for # ', count)\n",
    "    if qtr_date in manual:\n",
    "        master_table = extract_tables_manual(content, qtr_date)\n",
    "    else:\n",
    "        master_table = extract_tables(content, qtr_date)\n",
    "    print(\"Done creating master_table # \", count)\n",
    "    processed_table_ = process_table(\n",
    "        master_table, qtr_date.replace(',', ''))\n",
    "    processed_table_.to_excel(\n",
    "        writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "    # processed_table_.to_csv(\n",
    "        # '../csv_file/'+qtr_date.replace(',', '')+'.csv')\n",
    "    print(\"Done processed_table # \", count)\n",
    "    count += 1\n",
    "    writer.book.save(path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "### To test one file at a time\n",
    "#### You can use the index of the link to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = filing_links.iloc[-1]['Filings URL']\n",
    "# date = filing_links.iloc[-1]['Reporting date']\n",
    "# url, date\n",
    "# response = requests.get(url, headers=headers)\n",
    "# content = parse_and_trim(response.content, 'HTML')\n",
    "# master_table = extract_tables(content, date)\n",
    "# process_table_ = process_table(master_table, \"\")\n",
    "#process_table_.to_excel(\"example.xlsx\")\n",
    "#process_table_.to_csv('example.csv')\n",
    "# process_table_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../ARCC_Investment.xlsx'\n",
    "xls = pd.ExcelFile(path)\n",
    "all_sheets = pd.read_excel(path, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "# Loop through each sheet and create a DataFrame in the dictionary\n",
    "for sheet_name, sheet_df in all_sheets.items():\n",
    "    dataframes[sheet_name.replace(' ', '_')] = sheet_df\n",
    "for sheet_name, sheet_df in dataframes.items():\n",
    "    print(f\"DataFrame name: {sheet_name} : {sheet_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataframes = {}\n",
    "# Loop through each sheet and create a DataFrame in the dictionary\n",
    "for sheet_name, sheet_df in all_sheets.items():\n",
    "    dataframes[sheet_name.replace(' ', '_')] = sheet_df\n",
    "\n",
    "# Extract DataFrame names and shapes\n",
    "df_names = []\n",
    "df_shapes = []\n",
    "for sheet_name, sheet_df in dataframes.items():\n",
    "    df_names.append(sheet_name)\n",
    "    df_shapes.append(sheet_df.shape)\n",
    "\n",
    "# Create a bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_names, [shape[0] for shape in df_shapes], color='skyblue')\n",
    "plt.xlabel('Number of Rows')\n",
    "plt.ylabel('DataFrame Name')\n",
    "plt.title('DataFrame Shapes')\n",
    "plt.gca().invert_yaxis()  # Reverse the order of DataFrame names\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dataframes:\n",
    "    print(dataframes[dataframe].columns.values)\n",
    "\n",
    "print('Without none columns')\n",
    "for dataframe in dataframes:\n",
    "    # Get the columns with \"Unnamed\" in their names\n",
    "    unnamed_columns = [\n",
    "        col for col in dataframes[dataframe].columns if 'Unnamed' in col]\n",
    "    # Drop the unnamed columns\n",
    "    dataframes[dataframe].drop(columns=unnamed_columns, inplace=True)\n",
    "    # Print the updated columns\n",
    "    print(dataframes[dataframe].columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_pattern = {\n",
    "    r'^Company\\s*\\((\\d+)\\)$': 'Company',\n",
    "    r'^Interest\\(\\d+\\)\\(\\d+\\)$': 'Interest',\n",
    "    r'^Acquisition\\s+Date$': 'Acquisition Date',\n",
    "    r'^Amortized\\s+Cost$': 'Amortized Cost',\n",
    "    r'^Interest\\s+\\(\\d+\\)\\(\\d+\\)$': 'Interest',\n",
    "    r'^Fair\\s+Value$': 'Fair Value',\n",
    "    r'^Coupon\\s+\\(\\d+\\)$': 'Coupon',\n",
    "    r'^Reference\\s+\\(\\d+\\)$': 'Reference',\n",
    "    r'^Spread\\s+\\(\\d+\\)$': 'Spread',\n",
    "    r'^Maturity\\s+Date$': 'Maturity Date',\n",
    "    r'^Shares/Units$': 'Shares/Units',\n",
    "    r'^Principal$': 'Principal',\n",
    "}\n",
    "\n",
    "def rename_columns_with_pattern(df):\n",
    "    df.columns = df.columns.to_series().replace(column_pattern, regex=True)\n",
    "\n",
    "\n",
    "# Iterate through dataframes and apply the column renaming function\n",
    "for dataframe in dataframes:\n",
    "    rename_columns_with_pattern(dataframes[dataframe])\n",
    "\n",
    "# Print the updated columns for each dataframe\n",
    "for dataframe in dataframes:\n",
    "    print(dataframes[dataframe].columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../clean_csv_file\", exist_ok=True)\n",
    "\n",
    "excel_file_name = '../Clean_ARCC_Investment.xlsx'\n",
    "\n",
    "# Create an Excel writer object\n",
    "excel_writer = pd.ExcelWriter(excel_file_name, engine='openpyxl')\n",
    "\n",
    "# Iterate through dataframes and write them to the Excel file\n",
    "for idx, dataframe in enumerate(dataframes):\n",
    "    # Write each dataframe to a separate sheet (sheet names will be Sheet1, Sheet2, etc.)\n",
    "    dataframes[dataframe].to_excel(\n",
    "        excel_writer, sheet_name=f'{dataframe}', index=False)\n",
    "    dataframes[dataframe].to_csv('../clean_csv_file/'+dataframe+'.csv')\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer.book.save(excel_file_name)\n",
    "\n",
    "print(f'Excel file \"{excel_file_name}\" has been created.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL DONE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
