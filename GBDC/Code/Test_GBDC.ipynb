{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 8,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 8,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 8,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 8,
>>>>>>> 9e06f79 (sep17)
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_and_trim(content, content_type):\n",
    "    if content_type == 'HTML':\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    else:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        try:\n",
    "            tag.attrs = None\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    for linebreak in soup.find_all('br'):\n",
    "        linebreak.extract()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Form type</th>\n",
       "      <th>Form description</th>\n",
       "      <th>Filing date</th>\n",
       "      <th>Reporting date</th>\n",
       "      <th>Act</th>\n",
       "      <th>Film number</th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Filings URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>34</td>\n",
       "      <td>13810352</td>\n",
       "      <td>0001144204-13-026113</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2013-08-08</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>34</td>\n",
       "      <td>131020072</td>\n",
       "      <td>0001144204-13-043799</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-K</td>\n",
       "      <td>Annual report [Section 13 and 15(d), not S-K I...</td>\n",
       "      <td>2013-12-03</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>34</td>\n",
       "      <td>131254591</td>\n",
       "      <td>0001144204-13-065322</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>34</td>\n",
       "      <td>14578102</td>\n",
       "      <td>0001144204-14-006255</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>34</td>\n",
       "      <td>14823110</td>\n",
       "      <td>0001144204-14-028416</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Form type                                   Form description Filing date  \\\n",
       "0      10-Q            Quarterly report [Sections 13 or 15(d)]  2013-05-03   \n",
       "1      10-Q            Quarterly report [Sections 13 or 15(d)]  2013-08-08   \n",
       "2      10-K  Annual report [Section 13 and 15(d), not S-K I...  2013-12-03   \n",
       "3      10-Q            Quarterly report [Sections 13 or 15(d)]  2014-02-06   \n",
       "4      10-Q            Quarterly report [Sections 13 or 15(d)]  2014-05-08   \n",
       "\n",
       "  Reporting date  Act  Film number      Accession number  \\\n",
       "0     2013-03-31   34     13810352  0001144204-13-026113   \n",
       "1     2013-06-30   34    131020072  0001144204-13-043799   \n",
       "2     2013-09-30   34    131254591  0001144204-13-065322   \n",
       "3     2013-12-31   34     14578102  0001144204-14-006255   \n",
       "4     2014-03-31   34     14823110  0001144204-14-028416   \n",
       "\n",
       "                                         Filings URL  \n",
       "0  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "1  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "2  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "3  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "4  https://www.sec.gov/Archives/edgar/data/000147...  "
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 3,
=======
     "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
=======
     "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
=======
     "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
=======
     "execution_count": 9,
>>>>>>> 9e06f79 (sep17)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'GOLUB CAPITAL BDC, Inc.'\n",
    "}\n",
    "filing_links = pd.read_excel(\n",
    "    \"../GBDC__sec_filing_links.xlsx\", engine='openpyxl')\n",
    "filing_links.head(5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Form type</th>\n",
       "      <th>Form description</th>\n",
       "      <th>Filing date</th>\n",
       "      <th>Reporting date</th>\n",
       "      <th>Act</th>\n",
       "      <th>Film number</th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Filings URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>May 03, 2013</td>\n",
       "      <td>March 31, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>13810352</td>\n",
       "      <td>0001144204-13-026113</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>August 08, 2013</td>\n",
       "      <td>June 30, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>131020072</td>\n",
       "      <td>0001144204-13-043799</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-K</td>\n",
       "      <td>Annual report [Section 13 and 15(d), not S-K I...</td>\n",
       "      <td>December 03, 2013</td>\n",
       "      <td>September 30, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>131254591</td>\n",
       "      <td>0001144204-13-065322</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>February 06, 2014</td>\n",
       "      <td>December 31, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>14578102</td>\n",
       "      <td>0001144204-14-006255</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>May 08, 2014</td>\n",
       "      <td>March 31, 2014</td>\n",
       "      <td>34</td>\n",
       "      <td>14823110</td>\n",
       "      <td>0001144204-14-028416</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Form type                                   Form description  \\\n",
       "0      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "1      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "2      10-K  Annual report [Section 13 and 15(d), not S-K I...   \n",
       "3      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "4      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "\n",
       "         Filing date      Reporting date  Act  Film number  \\\n",
       "0       May 03, 2013      March 31, 2013   34     13810352   \n",
       "1    August 08, 2013       June 30, 2013   34    131020072   \n",
       "2  December 03, 2013  September 30, 2013   34    131254591   \n",
       "3  February 06, 2014   December 31, 2013   34     14578102   \n",
       "4       May 08, 2014      March 31, 2014   34     14823110   \n",
       "\n",
       "       Accession number                                        Filings URL  \n",
       "0  0001144204-13-026113  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "1  0001144204-13-043799  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "2  0001144204-13-065322  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "3  0001144204-14-006255  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "4  0001144204-14-028416  https://www.sec.gov/Archives/edgar/data/000147...  "
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 4,
=======
     "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
=======
     "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
=======
     "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
=======
     "execution_count": 10,
>>>>>>> 9e06f79 (sep17)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_columns = ['Filing date', 'Reporting date']\n",
    "for col in date_columns:\n",
    "    filing_links[col] = pd.to_datetime(filing_links[col], format='%Y-%m-%d')\n",
    "for col in date_columns:\n",
    "    filing_links[col] = filing_links[col].dt.strftime(\"%B %d, %Y\")\n",
    "filing_links.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filing_links = filing_links.drop(\n",
    "#     filing_links[filing_links['Reporting date'] == 'September 30, 2017'].index)\n",
    "# print(filing_links[filing_links['Reporting date'] == 'September 30, 2017'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/GBDC_Investment.xlsx'\n",
    "# writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content, qtr_date)\n",
    "#     master_table.to_excel(\n",
    "#         writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "#     writer.book.save(path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = 'https://www.sec.gov/Archives/edgar/data/1476765/000147676517000078/gbdc201710-k.htm'\n",
    "# date = 'September 30, 2017'\n",
    "# url, date\n",
    "# response = requests.get(url, headers=headers)\n",
    "# content = parse_and_trim(response.content, 'HTML')\n",
    "# master_table = extract_tables(content, date)\n",
    "# # process_table_ = process_table(master_table, \"\")\n",
    "# # process_table_.to_excel(\"example.xlsx\")\n",
    "# # process_table_.to_csv('example.csv')\n",
    "# # process_table_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index=-2\n",
    "# url, date = filing_links.iloc[index]['Filings URL'], filing_links.iloc[index]['Reporting date']\n",
    "# url='https://www.sec.gov/Archives/edgar/data/1476765/000162828016021522/gbdc201510-k.htm'\n",
    "# date='September 30, 2016'\n",
    "# print(url, date)\n",
    "# response = requests.get(url, headers=headers)\n",
    "# content = parse_and_trim(response.content, 'HTML')\n",
    "# search_texts = [\n",
    "#     'Consolidated Schedule of Investments',\n",
    "#     'Consolidated Schedule of Investments - (continued)',\n",
    "#         'Consolidated Schedule of Investments (unaudited) - (continued)',\n",
    "#         'Consolidated Schedule of Investments (unaudited)']\n",
    "# all_tags=content.find_all(text=search_texts)\n",
    "# for tag in all_tags:\n",
    "#     print(tag,tag.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     consolidated_schedule_regex = re.compile(r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "#     for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "#         print(\"Tag:\", tag)\n",
    "#         print(\"Next:\", tag.find_next())\n",
    "#         print(\"next:\", tag.next)\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content,qtr_date)\n",
    "#     # print(master_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "data_frames = []\n",
    "master_table = None\n",
    "url, date = filing_links.iloc[index]['Filings URL'], filing_links.iloc[index]['Reporting date']\n",
    "print(url, date)\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    print(\"Next:\", tag.next)\n",
    "    table = tag.find_next(\"table\")\n",
    "    if table:\n",
    "        # Extract the table data into a data frame\n",
    "        table_data = []\n",
    "        for row in table.find_all('tr'):\n",
    "            # Include header cells ('th') if necessary\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            row_data = [column.get_text(strip=True) for column in columns]\n",
    "            table_data.append(row_data)\n",
    "\n",
    "        # Create a data frame from the table data and add it to the list\n",
    "        table_df = pd.DataFrame(table_data)\n",
    "        data_frames.append(table_df)\n",
    "\n",
    "        if master_table is None:\n",
    "            master_table = table_df\n",
    "        else:\n",
    "            master_table = pd.concat(\n",
    "                [master_table, table_df], ignore_index=True)\n",
    "\n",
    "    # Print or process the data frames as needed\n",
    "    # for idx, df in enumerate(data_frames):\n",
    "    #     print(f\"Data Frame {idx + 1}:\\n\", df)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                table = tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    # Extract the table data into a data frame\n",
    "                    table_data = []\n",
    "                    for row in table.find_all('tr'):\n",
    "                        # Include header cells ('th') if necessary\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        row_data = [column.get_text(strip=True)\n",
    "                                    for column in columns]\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    # Create a data frame from the table data and add it to the list\n",
    "                    table_df = pd.DataFrame(table_data)\n",
    "                    if len(table_df.columns) > 10:\n",
    "                        data_frames.append(table_df)\n",
    "\n",
    "                        if master_table is None:\n",
    "                            master_table = table_df\n",
    "                        else:\n",
    "                            master_table = pd.concat(\n",
    "                                [master_table, table_df], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url, date)\n",
    "response = requests.get(url, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "master_table = extract_tables(content, date)\n",
    "print(master_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv(\"test.csv\")\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/2_Test_GBDC_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "    print(html_link, qtr_date)\n",
    "    response = requests.get(html_link, headers=headers)\n",
    "    content = parse_and_trim(response.content, 'HTML')\n",
    "    master_table = extract_tables(content, qtr_date)\n",
    "    master_table.to_excel(\n",
    "        writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "    writer.book .save(path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP 30 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sec.gov/Archives/edgar/data/1476765/000147676517000078/gbdc201710-k.htm\"\n",
    "date = \"September 30, 2017\"\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "contentDUP = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    print(\"Next_date:\", re.search(date_regex_pattern1, tag.find_next().text))\n",
    "    print(\"next:\", tag.next)\n",
    "    print(tag.find_next(\"table\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = extract_tables(content, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### June_30_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                table = tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    # Extract the table data into a data frame\n",
    "                    table_data = []\n",
    "                    for row in table.find_all('tr'):\n",
    "                        # Include header cells ('th') if necessary\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        row_data = [column.get_text(strip=True)\n",
    "                                    for column in columns]\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    # Create a data frame from the table data and add it to the list\n",
    "                    table_df = pd.DataFrame(table_data)\n",
    "                    if len(table_df.columns) > 10:\n",
    "\n",
    "                        if master_table is None:\n",
    "                            master_table = table_df\n",
    "                        else:\n",
    "                            master_table = pd.concat(\n",
    "                                [master_table, table_df], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"June 30, 2016\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "contentDUP = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = extract_tables(content, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv('test.csv')\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Find_next:\", tag.find_next())\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    print(\"Next_date:\", re.search(date_regex_pattern1, tag.find_next().text))\n",
    "    print(\"next:\", tag.next)\n",
    "    print(\"Next next: \", tag.find_next().next.next.next.text)\n",
    "    print(\"next sib: \", tag.find_next_sibling())\n",
    "    # print(tag.find_next(\"table\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "\n",
    "date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    print(\"next:\", tag.next)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag)\n",
    "    print(\"Next:\", tag.find_next())\n",
    "    print(\"Next:\", tag.next)\n",
    "    table = tag.find_next(\"table\")\n",
    "    if table:\n",
    "        # Extract the table data into a data frame\n",
    "        table_data = []\n",
    "        for row in table.find_all('tr'):\n",
    "            # Include header cells ('th') if necessary\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            row_data = [column.get_text(strip=True) for column in columns]\n",
    "            table_data.append(row_data)\n",
    "\n",
    "        # Create a data frame from the table data and add it to the list\n",
    "        table_df = pd.DataFrame(table_data)\n",
    "        data_frames.append(table_df)\n",
    "\n",
    "        if master_table is None:\n",
    "            master_table = table_df\n",
    "        else:\n",
    "            master_table = pd.concat(\n",
    "                [master_table, table_df], ignore_index=True)\n",
    "\n",
    "    # Print or process the data frames as needed\n",
    "    # for idx, df in enumerate(data_frames):\n",
    "    #     print(f\"Data Frame {idx + 1}:\\n\", df)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_texts = [\n",
    "    'Consolidated Schedule of Investments',\n",
    "    'Consolidated Schedule of Investments - (continued)',\n",
    "    'Consolidated Schedule of Investments (unaudited) - (continued)',\n",
    "    'Consolidated Schedule of Investments (unaudited)',\n",
    "    'Consolidated Schedule of Investments (unaudited) -\\n(continued)']\n",
    "all_tags = content.find_all(text=search_texts)\n",
    "for tag in all_tags:\n",
    "    print(tag, tag.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", \"w\") as file:\n",
    "    file.write(str(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECHECKING THE EXTRACTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    # master_table = master_table.applymap(\n",
    "    #     lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    # master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "    #     r'^\\s*\\$\\s*$', 0, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    # master_table = master_table.apply(lambda x: x.str.strip().replace(\n",
    "    #     '', np.nan) if x.dtype == \"object\" else x)\n",
    "\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "date = \"December 31, 2013\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "# webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "master_table = extract_tables(content, date)\n",
    "contentDUP = content\n",
    "master_tableDUP = master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv('test_2.csv')\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                table = tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    # Extract the table data into a data frame\n",
    "                    table_data = []\n",
    "                    for row in table.find_all('tr'):\n",
    "                        # Include header cells ('th') if necessary\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        row_data = [column.get_text(strip=True)\n",
    "                                    for column in columns]\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    # Create a data frame from the table data and add it to the list\n",
    "                    table_df = pd.DataFrame(table_data)\n",
    "                    new_table = table_df.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                    new_table = new_table.replace(\n",
    "                        r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                    table_df = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                    if len(table_df.columns) > 10:\n",
    "                        data_frames.append(table_df)\n",
    "\n",
    "                        if master_table is None:\n",
    "                            master_table = table_df\n",
    "                        else:\n",
    "                            master_table = pd.concat(\n",
    "                                [master_table, table_df], ignore_index=True)\n",
    "\n",
    "            # print(master_table)\n",
    "\n",
    "    # master_table = master_table.applymap(\n",
    "    #     lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    # master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "    #     r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "date = \"December 31, 2013\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "# webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "master_table = extract_tables(content, date)\n",
    "contentDUP = content\n",
    "master_tableDUP = master_table"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 30,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 30,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 30,
>>>>>>> 9e06f79 (sep17)
=======
   "execution_count": 30,
>>>>>>> 9e06f79 (sep17)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "December 31, 2015\n",
      "December 31, 2015 december312015 december312015\n"
=======
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
      "September 30, 2017\n",
      "September 30, 2017 september302017 september302017\n",
      "                                                    0                   1  \\\n",
      "2                                                 NaN  September 30, 2017   \n",
      "3                                              Assets                 NaN   \n",
      "4                          Investments, at fair value                 NaN   \n",
      "5    Non-controlled/non-affiliate company investments                 NaN   \n",
      "6        Non-controlled affiliate company investments                3707   \n",
      "7            Controlled affiliate company investments               95015   \n",
      "8   Total investments at fair value (amortized cos...             1685015   \n",
      "9                           Cash and cash equivalents                3988   \n",
      "10               Restricted cash and cash equivalents               58570   \n",
      "11                                Interest receivable                6271   \n",
      "12                                       Other assets                 332   \n",
      "13                                       Total Assets                 NaN   \n",
      "14                                        Liabilities                 NaN   \n",
      "15                                               Debt                 NaN   \n",
      "16               Less unamortized debt issuance costs                4273   \n",
      "17          Debt less unamortized debt issuance costs              776827   \n",
      "18  Secured borrowings, at fair value (proceeds of...                   0   \n",
      "19                                   Interest payable                3800   \n",
      "20              Management and incentive fees payable               13215   \n",
      "21              Accounts payable and accrued expenses                2312   \n",
      "22                               Accrued trustee fees                  76   \n",
      "23                                  Total Liabilities              796230   \n",
      "24             Commitments and Contingencies (Note 8)                 NaN   \n",
      "25                                         Net Assets                 NaN   \n",
      "26  Preferred stock, par value $0.001 per share, 1...                   0   \n",
      "27  Common stock, par value $0.001 per share, 100,...                  60   \n",
      "28                   Paid in capital in excess of par              939307   \n",
      "29                Undistributed net investment income                1954   \n",
      "30  Net unrealized appreciation (depreciation) on ...               16444   \n",
      "31            Net realized gain (loss) on investments                 181   \n",
      "32                                   Total Net Assets              957946   \n",
      "33             Total Liabilities and Total Net Assets                 NaN   \n",
      "34                Number of common shares outstanding            59577293   \n",
      "35                   Net asset value per common share                 NaN   \n",
      "\n",
      "          2                   3         4    5        6   7  \n",
      "2       NaN  September 30, 2016       NaN  NaN      NaN NaN  \n",
      "3       NaN                 NaN       NaN  NaN      NaN NaN  \n",
      "4       NaN                 NaN       NaN  NaN      NaN NaN  \n",
      "5   1586293                 NaN       NaN  NaN  1546766 NaN  \n",
      "6       NaN                 NaN      9618  NaN      NaN NaN  \n",
      "7       NaN                 NaN    104228  NaN      NaN NaN  \n",
      "8       NaN                 NaN   1660612  NaN      NaN NaN  \n",
      "9       NaN                 NaN     10947  NaN      NaN NaN  \n",
      "10      NaN                 NaN     78593  NaN      NaN NaN  \n",
      "11      NaN                 NaN      5935  NaN      NaN NaN  \n",
      "12      NaN                 NaN       422  NaN      NaN NaN  \n",
      "13  1754176                 NaN       NaN  NaN  1756509 NaN  \n",
      "14      NaN                 NaN       NaN  NaN      NaN NaN  \n",
      "15   781100                 NaN       NaN  NaN   864700 NaN  \n",
      "16      NaN                 NaN      5627  NaN      NaN NaN  \n",
      "17      NaN                 NaN    859073  NaN      NaN NaN  \n",
      "18      NaN                 NaN       475  NaN      NaN NaN  \n",
      "19      NaN                 NaN      3229  NaN      NaN NaN  \n",
      "20      NaN                 NaN     12763  NaN      NaN NaN  \n",
      "21      NaN                 NaN      2072  NaN      NaN NaN  \n",
      "22      NaN                 NaN        72  NaN      NaN NaN  \n",
      "23      NaN                 NaN    877684  NaN      NaN NaN  \n",
      "24      NaN                 NaN       NaN  NaN      NaN NaN  \n",
      "25      NaN                 NaN       NaN  NaN      NaN NaN  \n",
      "26      NaN                 NaN         0  NaN      NaN NaN  \n",
      "27      NaN                 NaN        55  NaN      NaN NaN  \n",
      "28      NaN                 NaN    855998  NaN      NaN NaN  \n",
      "29      NaN                 NaN     18832  NaN      NaN NaN  \n",
      "30      NaN                 NaN     13104  NaN      NaN NaN  \n",
      "31      NaN                 NaN    (9,164    )      NaN NaN  \n",
      "32      NaN                 NaN    878825  NaN      NaN NaN  \n",
      "33  1754176                 NaN       NaN  NaN  1756509 NaN  \n",
      "34      NaN                 NaN  55059067  NaN      NaN NaN  \n",
      "35    16.08                 NaN       NaN  NaN    15.96 NaN  \n"
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_8602/380200694.py:19: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
=======
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_45198/115308254.py:21: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
>>>>>>> 9e06f79 (sep17)
=======
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_45198/115308254.py:21: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
>>>>>>> 9e06f79 (sep17)
=======
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_45198/115308254.py:21: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
>>>>>>> 9e06f79 (sep17)
=======
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_45198/115308254.py:21: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
>>>>>>> 9e06f79 (sep17)
      "  for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n"
     ]
    },
    {
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "(505, 27)\n"
=======
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:155\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb Cell 37\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url\u001b[39m=\u001b[39murl, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m content \u001b[39m=\u001b[39m parse_and_trim(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mHTML\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m master_table \u001b[39m=\u001b[39m extract_tables(content, date)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m contentDUP \u001b[39m=\u001b[39m content\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m master_tableDUP \u001b[39m=\u001b[39m master_table\n",
      "\u001b[1;32m/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb Cell 37\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(master_table)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mif\u001b[39;00m qtr_date_cleaned \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mseptember302017\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m master_table \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     match \u001b[39m=\u001b[39m master_table\u001b[39m.\u001b[39;49mloc[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         master_table\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m             \u001b[39mlambda\u001b[39;49;00m row: re\u001b[39m.\u001b[39;49msearch(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39mTotal investments and cash\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mstr\u001b[39;49m(row)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         )\u001b[39m.\u001b[39;49mnotna()\u001b[39m.\u001b[39;49many()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match\u001b[39m.\u001b[39mempty:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fuadhassan/Desktop/BDC_RA/GBDC/Code/Test_GBDC.ipynb#X51sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1343\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexing.py:1293\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:4095\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4093\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[1;32m   4094\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4095\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4097\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m   4098\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
     ]
    }
   ],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "    try:\n",
    "        if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "            consolidated_schedule_regex = re.compile(\n",
    "                r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "        else:\n",
    "            consolidated_schedule_regex = re.compile(\n",
    "                r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
=======
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
    "\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$|'\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$'\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
    "\n",
>>>>>>> 9e06f79 (sep17)
=======
    "\n",
>>>>>>> 9e06f79 (sep17)
=======
    "\n",
>>>>>>> 9e06f79 (sep17)
=======
    "\n",
>>>>>>> 9e06f79 (sep17)
    "                html_table = tag.find_next('table')\n",
    "\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), keep_default_na=False, skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '0').replace('%', '')) if type(x) == str else x)\n",
    "\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
    "                print(master_table)\n",
    "\n",
    "                if qtr_date_cleaned == 'september302017' and master_table is not None:\n",
    "                    match = master_table.loc[\n",
    "                        master_table.apply(\n",
    "                            lambda row: re.search(\n",
    "                                'Total investments and cash', str(row)),\n",
    "                            axis=1\n",
    "                        ).notna().any()\n",
    "                    ]\n",
    "                    if not match.empty:\n",
    "                        break\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
    "            # print(master_table)\n",
    "\n",
    "            master_table = master_table.replace('N/A', 'No Value')\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "date = \"December 31, 2015\"\n",
=======
    "date = 'September 30, 2017'\n",
>>>>>>> 9e06f79 (sep17)
=======
    "date = 'September 30, 2017'\n",
>>>>>>> 9e06f79 (sep17)
=======
    "date = 'September 30, 2017'\n",
>>>>>>> 9e06f79 (sep17)
=======
    "date = 'September 30, 2017'\n",
>>>>>>> 9e06f79 (sep17)
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "# webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "\n",
    "master_table = extract_tables(content, date)\n",
    "contentDUP = content\n",
    "master_tableDUP = master_table\n",
    "master_table.to_csv('test.csv')\n",
    "with open('text.txt', 'w') as f:\n",
    "    f.write(str(content))\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "filing_links = filing_links[filing_links['Reporting date']\n",
    "                            != 'September 30, 2017']\n",
=======
    "# filing_links = filing_links[filing_links['Reporting date']\n",
    "#                             != 'September 30, 2017']\n",
>>>>>>> 9e06f79 (sep17)
=======
    "# filing_links = filing_links[filing_links['Reporting date']\n",
    "#                             != 'September 30, 2017']\n",
>>>>>>> 9e06f79 (sep17)
=======
    "# filing_links = filing_links[filing_links['Reporting date']\n",
    "#                             != 'September 30, 2017']\n",
>>>>>>> 9e06f79 (sep17)
=======
    "# filing_links = filing_links[filing_links['Reporting date']\n",
    "#                             != 'September 30, 2017']\n",
>>>>>>> 9e06f79 (sep17)
    "\n",
    "# This does the job\n",
    "# path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/Master_tables_GBDC_Investment.xlsx'\n",
    "# if not os.path.exists('../MT_csv_file'):\n",
    "#     os.makedirs('../MT_csv_file')\n",
    "\n",
    "# writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content, qtr_date)\n",
    "#     master_table.to_excel(\n",
    "#         writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "#     master_table.to_csv(\n",
    "#         '../MT_csv_file/'+qtr_date.replace(',', '')+'.csv')\n",
    "#     writer.book .save(path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", we have one second lien investment and one subordinated debt investment as shown in the Consolidated Schedule of Investments.\n",
      "Consolidated Schedules of Investments as of September 30, 2017 and 2016\n",
      "We have audited the accompanying consolidated statements of financial condition, including the consolidated schedules of investments, of Golub Capital BDC, Inc. and Subsidiaries (collectively, the “Company”) as of September 30, 2017 and 2016, and the related consolidated statements of operations, changes in net assets and cash flows for each of the two years in the period ended September 30, 2017. These financial statements are the responsibility of the Company’s management. Our responsibility is to express an opinion on these financial statements based on our audits.\n",
      "We also have audited, in accordance with the standards of the Public Company Accounting Oversight Board (United States), the consolidated statements of financial condition, including the consolidated schedules of investments, of Golub Capital BDC, Inc. and Subsidiaries as of September 30, 2017 and 2016, and the related consolidated statements of operations, changes in net assets and cash flows for each of the two years in the period ended September 30, 2017 of Golub Capital BDC, Inc. and Subsidiaries and our report dated November 20, 2017 expressed an unqualified opinion thereon. \n",
      "Consolidated Schedule of Investments\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      ", and therefore, the entire one stop loan asset remains in the Consolidated Schedule of Investments.  (See Note 6 in the accompanying notes to the consolidated financial statements.)\n",
      "Consolidated Schedule of Investments - (continued)\n",
      "Refer to the Consolidated Schedules of Investments for further details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_45198/3970588305.py:9: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for tag in content.find_all(text=re.compile(consolidated_schedule_regex)):\n"
     ]
    }
   ],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$|'\n",
    "    r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$'\n",
    ")\n",
    "\n",
    "\n",
    "date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "# date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "for tag in content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "    date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "    print(tag)"
   ]
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
=======
>>>>>>> 9e06f79 (sep17)
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
