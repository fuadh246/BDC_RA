{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_trim(content, content_type):\n",
    "    if content_type == 'HTML':\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    else:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        try:\n",
    "            tag.attrs = None\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    for linebreak in soup.find_all('br'):\n",
    "        linebreak.extract()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(string):\n",
    "    pattern = r'\\s+'\n",
    "    replaced_string = re.sub(pattern, ' ', string)\n",
    "    return replaced_string\n",
    "\n",
    "\n",
    "def find_qrt_date(content):\n",
    "    qtr_date = content.find_all(text=re.compile(\n",
    "        r'for\\s+(the\\s+)?(fiscal\\s+)?year\\s+ended\\s+|for\\s+the\\s+quarter\\s+ended\\s+|for\\s+the\\s+quarterly\\s+period\\s+ended\\s+', re.IGNORECASE))\n",
    "    qtr_match = re.search(\n",
    "        r'([A-Za-z]+)\\s+(\\d{1,2}),\\s+(\\d{4})', qtr_date[0].replace('\\n', ''))\n",
    "    if qtr_match is None:\n",
    "        qtr_match = qtr_match = re.search(\n",
    "            r'([A-Za-z]+) (\\d{1,2}), (\\d{4})', qtr_date[1])\n",
    "    return remove_multiple_spaces(str(qtr_match.group()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'ARES CAPITAL CORP'\n",
    "}\n",
    "filing_links = pd.read_excel(\n",
    "    \"/Users/fuadhassan/Desktop/BDC_RA/ARCC/ARCC__sec_filing_links.xlsx\")\n",
    "filing_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops all the amendment filing\n",
    "filing_links = filing_links.drop(filing_links[filing_links['Form description'].str.contains(\n",
    "    'amendment', case=False)].index).reset_index(drop=True)\n",
    "filing_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['Filing date', 'Reporting date']\n",
    "for col in date_columns:\n",
    "    filing_links[col] = pd.to_datetime(filing_links[col], format='%Y-%m-%d')\n",
    "for col in date_columns:\n",
    "    filing_links[col] = filing_links[col].dt.strftime(\"%B %d, %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_links.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = filing_links.iloc[1]['Filings URL']\n",
    "# date = filing_links.iloc[1]['Reporting date']\n",
    "# url, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(url, headers=headers)\n",
    "# content = parse_and_trim(response.content, 'HTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real one on git hub\n",
    "\n",
    "\n",
    "# def extract_tables(soup_content, qtr_date):\n",
    "#     master_table = None\n",
    "#     all_tags = soup_content.find_all(True)\n",
    "#     print(type(all_tags))\n",
    "#     count = 0\n",
    "#     for tag in soup_content.find_all(text=re.compile('^.*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS.*$')):\n",
    "#         print('yes')\n",
    "#         date_str = re.search(r'([A-Za-z]+) (\\d{1,2}), (\\d{4})', tag)\n",
    "#         print(date_str)\n",
    "#         if date_str is None:\n",
    "#             next_line = tag.find_next(text=re.compile(\n",
    "#                 r'([A-Za-z]+) (\\d{1,2}), (\\d{4})')).text\n",
    "#             date_str = re.search(r'([A-Za-z]+) (\\d{1,2}), (\\d{4})', next_line)\n",
    "#         if date_str is None:\n",
    "#             next_line = tag.next.next.next.next.next.next.text\n",
    "#             date_str = re.search(r'([A-Za-z]+) (\\d{1,2}), (\\d{4})', next_line)\n",
    "#         if date_str is not None:\n",
    "#             date_str = str(date_str.group())\n",
    "#             date_str = unicodedata.normalize('NFKD', date_str)\n",
    "#             print(date_str)\n",
    "#             if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "#                 count += 1\n",
    "#                 print('Table found: ')\n",
    "#                 print('Table #', count)\n",
    "#                 html_table = tag.find_next('table')\n",
    "#                 if master_table is None:\n",
    "#                     master_table = pd.read_html(\n",
    "#                         html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "#                     master_table = master_table.applymap(lambda x: unicodedata.normalize(\n",
    "#                         'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "#                     master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "#                                                                                               regex=True)\n",
    "#                     master_table = master_table.dropna(how='all', axis=0)\n",
    "#                 else:\n",
    "#                     new_table = pd.read_html(\n",
    "#                         html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "#                     new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "#                         'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "#                     new_table = new_table.replace(r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan,\n",
    "#                                                                                         regex=True)\n",
    "#                     new_table = new_table.dropna(how='all', axis=0)\n",
    "#                     # print('head')\n",
    "#                     # print(new_table.head()) # text\n",
    "#                     master_table = master_table.append(\n",
    "#                         new_table.dropna(how='all', axis=0).reset_index(\n",
    "#                             drop=True).drop(index=0),\n",
    "#                         ignore_index=True)\n",
    "\n",
    "#     master_table = master_table.applymap(\n",
    "#         lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "#     master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "#         r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "#     return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    all_tags = soup_content.find_all(True)\n",
    "    count = 0\n",
    "    for tag in soup_content.find_all(text=re.compile('^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')):\n",
    "        next_line_text = tag.next.text.strip()\n",
    "        regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "        date_str = re.search(regex_pattern, next_line_text).group(1)\n",
    "\n",
    "        if date_str is None:\n",
    "            next_line = tag.find_next(text=re.compile(regex_pattern)).text\n",
    "            date_str = re.search(regex_pattern, next_line)\n",
    "\n",
    "        if date_str is None:\n",
    "            next_line = tag.next.text.strip()\n",
    "            date_str = re.search(\n",
    "                regex_pattern, next_line).group(1)\n",
    "\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str)\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "\n",
    "            if qtr_date.replace(',', '').strip().lower() in date_str.replace(',', '').strip().lower():\n",
    "                count += 1\n",
    "                print('Table #', count)\n",
    "                html_table = tag.find_next('table')\n",
    "\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('—', '-')) if type(x) == str else x)\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(soi_table_df, append_str):\n",
    "    soi_table_df = soi_table_df.replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "    soi_table_df = soi_table_df.dropna(how='all', axis=1)\n",
    "    soi_table_df = soi_table_df.dropna(\n",
    "        how='all', axis=0).reset_index(drop=True)\n",
    "    # print('1: ' + str(soi_table_df.shape))\n",
    "\n",
    "    # Separate header and data\n",
    "    soi_table_header = soi_table_df.iloc[0].dropna(how='any')\n",
    "    print('header: ')\n",
    "    # print(soi_table_header)\n",
    "    soi_table_data_df = soi_table_df.rename(\n",
    "        columns=soi_table_df.iloc[0]).drop(soi_table_df.index[0])\n",
    "    # print('2: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop rows with only two non-null values\n",
    "    # soi_table_data_df = soi_table_data_df.dropna(thresh=3)\n",
    "    # print('4: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # columns_to_fill = ['Amortized Cost', 'Fair Value']\n",
    "    # for col in columns_to_fill:\n",
    "    #     col_index = soi_table_data_df.columns.get_loc(col)\n",
    "    #     next_col_index = col_index + 1\n",
    "    #     for i in range(len(soi_table_data_df)):\n",
    "    #         current_value = soi_table_data_df.iat[i, col_index]\n",
    "\n",
    "    #         if pd.isna(current_value) and next_col_index < len(soi_table_data_df.columns):\n",
    "    #             next_valid_index = next((j for j, v in enumerate(\n",
    "    #                 soi_table_data_df.iloc[i, next_col_index:], start=next_col_index) if pd.notna(v)), None)\n",
    "\n",
    "    #             if next_valid_index is not None:\n",
    "    #                 next_value = soi_table_data_df.iat[i, next_valid_index]\n",
    "    #                 soi_table_data_df.iat[i, col_index] = next_value\n",
    "    #                 soi_table_data_df.iat[i, next_valid_index] = pd.NA\n",
    "\n",
    "    columns_to_fill = ['Amortized Cost', 'Fair  Value']\n",
    "\n",
    "    for col in columns_to_fill:\n",
    "        col_name_no_spaces = col.strip()\n",
    "        if col_name_no_spaces in soi_table_data_df.columns.str.strip():\n",
    "            col_index = soi_table_data_df.columns.get_loc(col_name_no_spaces)\n",
    "            next_col_index = col_index + 1\n",
    "\n",
    "            for i in range(len(soi_table_data_df)):\n",
    "                current_value = soi_table_data_df.iat[i, col_index]\n",
    "\n",
    "                if pd.isna(current_value) and next_col_index < len(soi_table_data_df.columns):\n",
    "                    next_valid_index = next((j for j, v in enumerate(\n",
    "                        soi_table_data_df.iloc[i, next_col_index:], start=next_col_index) if pd.notna(v)), None)\n",
    "\n",
    "                    if next_valid_index is not None:\n",
    "                        next_value = soi_table_data_df.iat[i, next_valid_index]\n",
    "                        soi_table_data_df.iat[i, col_index] = next_value\n",
    "                        soi_table_data_df.iat[i, next_valid_index] = pd.NA\n",
    "\n",
    "    # Drop rows labeled as subtotals\n",
    "    # subtotal_rows = soi_table_data_df[soi_table_data_df['Company (1)'].str.replace(' ', '').str.contains(\n",
    "    #     'subtotal', case=False, na=False)]\n",
    "    # soi_table_data_df = soi_table_data_df[~soi_table_data_df.index.isin(\n",
    "    #     subtotal_rows.index)]\n",
    "    # print('3: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop rows based on regex pattern (e.g., 'subtotal' or 'total')\n",
    "    pattern = r'^([Ss]ubtotal)|([Tt]otal)'\n",
    "    mask = soi_table_data_df.apply(lambda row: row.astype(\n",
    "        str).str.contains(pattern, case=False, na=False)).any(axis=1)\n",
    "    soi_table_data_df = soi_table_data_df[~mask]\n",
    "    # print('4: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Drop rows with all missing values\n",
    "    soi_table_df = soi_table_df.dropna(how='all')\n",
    "    # print('5: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    soi_table_data_df.to_excel('test.xlsx')\n",
    "\n",
    "    # # Drop columns with all missing values\n",
    "    soi_table_data_df = soi_table_data_df.dropna(how='all', axis=1)\n",
    "    # print('6: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # Forward fill the first two columns\n",
    "    col_indices = [0, 1]\n",
    "    soi_table_data_df.iloc[:, col_indices] = soi_table_data_df.iloc[:, col_indices].fillna(\n",
    "        method='ffill')\n",
    "    # print('7: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    # soi_table_data_df = soi_table_data_df.fillna(0)\n",
    "    # soi_table_data_df = soi_table_data_df.dropna(how='all', axis=1)\n",
    "\n",
    "    cols_to_convert = ['Principal', 'Amortized Cost', 'Fair Value']\n",
    "    for col_name in cols_to_convert:\n",
    "        if col_name in soi_table_data_df.columns:\n",
    "            # print(soi_table_data_df.dtypes[col_name])\n",
    "            soi_table_data_df[col_name] = pd.to_numeric(\n",
    "                soi_table_data_df[col_name], errors='coerce').fillna(0)\n",
    "\n",
    "    # columns_to_drop = []\n",
    "    # for column in soi_table_data_df.columns:\n",
    "    #     # Check for NaN values in the column\n",
    "    #     # Use .item() to get a single boolean value\n",
    "    #     if soi_table_data_df[column].isna().any().item():\n",
    "    #         columns_to_drop.append(column)\n",
    "\n",
    "    # soi_table_data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    soi_table_data_df = soi_table_data_df.replace('-', 0, regex=False)\n",
    "    print('8: ' + str(soi_table_data_df.shape))\n",
    "\n",
    "    return soi_table_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "path = '/Users/fuadhassan/Desktop/BDC_RA/ARCC/ARCC_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "\n",
    "for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "    print('start')\n",
    "    response = requests.get(html_link, headers=headers)\n",
    "    content = parse_and_trim(response.content, 'HTML')\n",
    "    print('content DONE')\n",
    "    master_table = extract_tables(content, qtr_date)\n",
    "    print(count, \"master_table DONE\")\n",
    "    processed_table_ = process_table(\n",
    "        master_table, qtr_date.replace(' ', '').replace(',', ''))\n",
    "    # print(processed_table_)\n",
    "    processed_table_.to_excel(\n",
    "        writer, sheet_name=qtr_date.replace(' ', '').replace(',', ''), index=False)\n",
    "    print(count, \"processed_table_ DONE\")\n",
    "    count += 1\n",
    "    writer.save()\n",
    "    print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "# writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = filing_links.iloc[27]['Filings URL']\n",
    "date = filing_links.iloc[27]['Reporting date']\n",
    "url, date\n",
    "# response = requests.get(url, headers=headers)\n",
    "# content = parse_and_trim(response.content, 'HTML')\n",
    "# master_table = extract_tables(content, date)\n",
    "# process_table_ = process_table(master_table, \"\")\n",
    "# process_table_.to_excel(\"ex.xlsx\")\n",
    "# process_table_.to_csv('ex.csv')\n",
    "# process_table_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
