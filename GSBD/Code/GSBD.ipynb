{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from openpyxl import workbook\n",
    "import re\n",
    "import os\n",
    "import webbrowser\n",
    "import unicodedata\n",
    "from Helper_package import Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ../GSBC_sec_filing_links.xlsx\n",
      "Filing link recived\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Goldman Sachs BDC, Inc. GSBD on NYSE'\n",
    "}\n",
    "CIK = '0001572694'\n",
    "\n",
    "filing_data = Helper.fetch_filing_data(cik=CIK, headers=headers)\n",
    "if filing_data is not None:\n",
    "    # Write DataFrame to Excel file with auto-adjusting column widths\n",
    "    file_name = \"../GSBC_sec_filing_links.xlsx\"\n",
    "    with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n",
    "        filing_data.to_excel(writer, index=False)\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        for i, col in enumerate(filing_data.columns):\n",
    "            column_len = max(filing_data[col].astype(\n",
    "                str).str.len().max(), len(col)) + 2\n",
    "            worksheet.set_column(i, i, column_len)\n",
    "\n",
    "    print(f\"Data written to {file_name}\")\n",
    "\n",
    "filing_links = Helper.get_filing_links('../GSBC_sec_filing_links.xlsx')\n",
    "print(\"Filing link recived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   accessionNumber        36 non-null     object \n",
      " 1   filingDate             36 non-null     object \n",
      " 2   reportDate             36 non-null     object \n",
      " 3   acceptanceDateTime     36 non-null     object \n",
      " 4   act                    36 non-null     int64  \n",
      " 5   form                   36 non-null     object \n",
      " 6   fileNumber             36 non-null     object \n",
      " 7   filmNumber             36 non-null     int64  \n",
      " 8   items                  0 non-null      float64\n",
      " 9   size                   36 non-null     int64  \n",
      " 10  isXBRL                 36 non-null     int64  \n",
      " 11  isInlineXBRL           36 non-null     int64  \n",
      " 12  primaryDocument        36 non-null     object \n",
      " 13  primaryDocDescription  36 non-null     object \n",
      " 14  fileLink               36 non-null     object \n",
      " 15  txtFileLink            36 non-null     object \n",
      "dtypes: float64(1), int64(5), object(10)\n",
      "memory usage: 4.6+ KB\n"
     ]
    }
   ],
   "source": [
    "filing_links.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date was converted to '%B %d, %Y' format and back to\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Series name: reportDate\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "36 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 416.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "filing_links['reportDate'] = pd.to_datetime(\n",
    "    filing_links['reportDate']).dt.strftime(\"%B %d, %Y\")\n",
    "print(\"Date was converted to '%B %d, %Y' format and back to\")\n",
    "filing_links['reportDate'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting the extraction function again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing 1 response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidated_schedule_regex = re.compile(\n",
    "#     r'(?i)^\\s*.*\\s*SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "# for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "#     print(\"Tag:\", tag)\n",
    "#     print(\"Find_next:\", tag.find_next())\n",
    "#     date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "#     print(\"Next_date:\", re.search(date_regex_pattern1, tag.find_next().text))\n",
    "#     print(\"next:\", tag.next)\n",
    "#     print(\"Next next: \", tag.find_next().next.next.next.text)\n",
    "#     print(\"next sib: \", tag.find_next_sibling())\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidated_schedule_regex = re.compile(\n",
    "#     r'(?i)^\\s*.*\\s*SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "# date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "\n",
    "# for index, (url, reporting_date) in enumerate(zip(filing_links['url'], filing_links['Reporting date'])):\n",
    "#     response = helper.get_response(url=url, headers=headers)\n",
    "#     content = helper.get_content(response)\n",
    "#     for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "\n",
    "#         print(reporting_date, \"Tag:\", tag)\n",
    "#         # print(\"Find_next:\", tag.find_next())\n",
    "#         print(reporting_date, \"Next_date:\", re.search(\n",
    "#             date_regex_pattern1, tag.text))\n",
    "#         # print(\"next:\", tag.next)\n",
    "#         # print(\"Next next: \", tag.find_next().next.next.next.text)\n",
    "#         # print(\"next sib: \", tag.find_next_sibling())\n",
    "#         # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filing_links = filing_links.drop(\n",
    "#     filing_links[filing_links['Reporting date'] == 'December 31, 2017'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cell_value(x):\n",
    "    \"\"\"\n",
    "    Clean the cell value by normalizing Unicode, stripping leading/trailing spaces, and replacing specific characters.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        replacement_dict = {'—': '0', '%': ' ', '  ': '', '': ''}\n",
    "        for old_char, new_char in replacement_dict.items():\n",
    "            x = x.replace(old_char, new_char)\n",
    "        x = unicodedata.normalize('NFKD', x.strip().strip('\\u200b'))\n",
    "    return x\n",
    "\n",
    "\n",
    "def extract_tables(content, qtr_date) -> pd.DataFrame:\n",
    "    master_table = None\n",
    "    # print(\"Now doing : \", qtr_date)\n",
    "    print(\"Currect file \" + qtr_date)\n",
    "\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    for tag in content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        try:\n",
    "            date_str = re.search(date_regex_pattern, tag.text)\n",
    "        except Exception as e:\n",
    "            print(f'Could not find date on extract_tables() : {e}')\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date = unicodedata.normalize('NFKD', qtr_date)\n",
    "            # print(f'{qtr_date} : {date_str} : {qtr_date==date_str}')\n",
    "\n",
    "            if qtr_date == date_str:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), keep_default_na=False, skiprows=0, flavor='bs4')[0]\n",
    "                # new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                #     'NFKD', x.strip().strip(u'\\u200b').replace('—', '0').replace('%', '').replace('(', '').replace(')', '')) if type(x) == str else x)\n",
    "\n",
    "                new_table = new_table.applymap(clean_cell_value)\n",
    "\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "    try:\n",
    "        master_table = master_table.applymap(\n",
    "            lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "        master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "            r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'url'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/fuadhassan/Desktop/BDC_RA/GSBD/Master_tables_GSBD_Investment.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m writer \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mExcelWriter(path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, (url, reporting_date) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mfiling_links\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, filing_links[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReporting date\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         content \u001b[38;5;241m=\u001b[39m Helper\u001b[38;5;241m.\u001b[39mget_content(\n\u001b[1;32m      6\u001b[0m             Helper\u001b[38;5;241m.\u001b[39mget_response(url\u001b[38;5;241m=\u001b[39murl, headers\u001b[38;5;241m=\u001b[39mheaders))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'url'"
     ]
    }
   ],
   "source": [
    "path = '/Users/fuadhassan/Desktop/BDC_RA/GSBD/Master_tables_GSBD_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for index, (url, reporting_date) in enumerate(zip(filing_links['url'], filing_links['Reporting date'])):\n",
    "    try:\n",
    "        content = Helper.get_content(\n",
    "            Helper.get_response(url=url, headers=headers))\n",
    "    except Exception as e:\n",
    "        print(f'failed to get the content: {e}')\n",
    "\n",
    "    master_table = extract_tables(content, reporting_date)\n",
    "    master_table.to_csv(\n",
    "        '../MT_csv_files/'+reporting_date.replace(',', '')+'.csv')\n",
    "    master_table.to_excel(\n",
    "        writer, sheet_name=reporting_date.replace(',', ''), index=False)\n",
    "    writer.book .save(path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# December 31, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = Helper.get_file_url('December 31, 2020', filing_links)\n",
    "content = Helper.get_content(Helper.get_response(url=url, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: Consolidated Schedules of Investments as of December 31, 2020 and 2019\n",
      "Next_date: <re.Match object; span=(44, 61), match='December\\xa031, 2020'>\n",
      "Next next:  \n",
      "next sib:  None\n",
      "Tag: We have audited the accompanying consolidated statements of assets and liabilities, including the consolidated schedules of investments of Goldman Sachs BDC, Inc. and its subsidiaries (the “Company”) as of December 31, 2020 and 2019, and the related consolidated statements of operations, of changes in net assets and of cash flows for each of the three years in the period ended December 31, 2020, including the related notes (collectively referred to as the “consolidated financial statements”). We also have audited the Company's internal control over financial reporting as of December 31, 2020, based on criteria established in \n",
      "Next_date: <re.Match object; span=(206, 223), match='December 31, 2020'>\n",
      "Next next:  Internal Control - Integrated Framework\n",
      "next sib:  <font>Internal Control - Integrated Framework</font>\n",
      "Tag: We have also previously audited, in accordance with the standards of the Public Company Accounting Oversight Board (United States), the consolidated statements of assets and liabilities, including the consolidated schedules of investments, of the Company as of December 31, 2018, 2017, 2016, 2015 and 2014, and the related consolidated statements of operations, of changes in net assets and of cash flows for the years ended December 31, 2017, 2016, 2015 and 2014 (none of which are presented herein), and we expressed unqualified opinions on those consolidated financial statements. In our opinion, the information set forth in the Senior Securities table of the Company as of December 31, 2020, 2019, 2018, 2017, 2016, 2015 and 2014, appearing on page 55, is fairly stated, in all material respects, in relation to the consolidated financial statements from which it has been derived.\n",
      "Next_date: <re.Match object; span=(261, 278), match='December 31, 2018'>\n",
      "Next next:  Basis for Opinions\n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2020 \n",
      "Next_date: <re.Match object; span=(43, 60), match='December 31, 2020'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of \n",
      "Next_date: None\n",
      "Next next:  December 31\n",
      "next sib:  <font>December 31</font>\n",
      "Tag: Consolidated Schedule of Investments as of \n",
      "Next_date: None\n",
      "Next next:  December 31\n",
      "next sib:  <font>December 31</font>\n",
      "Tag: Consolidated Schedule of Investments as of \n",
      "Next_date: None\n",
      "Next next:  December 31\n",
      "next sib:  <font>December 31</font>\n",
      "Tag: Consolidated Schedule of Investments as of \n",
      "Next_date: None\n",
      "Next next:  December 31\n",
      "next sib:  <font>December 31</font>\n",
      "Tag: Consolidated Schedule of Investments as of \n",
      "Next_date: None\n",
      "Next next:  December 31\n",
      "next sib:  <font>December 31</font>\n",
      "Tag: Consolidated Schedule of Investments as of \n",
      "Next_date: None\n",
      "Next next:  December 31\n",
      "next sib:  <font>December 31</font>\n",
      "Tag: Consolidated Schedule of Investments as of \n",
      "Next_date: None\n",
      "Next next:  December 31\n",
      "next sib:  <font>December 31</font>\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2019 \n",
      "Next_date: <re.Match object; span=(43, 60), match='December\\xa031, 2019'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2019 (continued) \n",
      "Next_date: <re.Match object; span=(43, 60), match='December 31, 2019'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2019 (continued) \n",
      "Next_date: <re.Match object; span=(43, 60), match='December 31, 2019'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2019 (continued) \n",
      "Next_date: <re.Match object; span=(43, 60), match='December 31, 2019'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2019 (continued) \n",
      "Next_date: <re.Match object; span=(43, 60), match='December 31, 2019'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2019 (continued) \n",
      "Next_date: <re.Match object; span=(43, 60), match='December 31, 2019'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: Consolidated Schedule of Investments as of December 31, 2019 (continued) \n",
      "Next_date: <re.Match object; span=(43, 60), match='December 31, 2019'>\n",
      "Next next:  (in thousands, except share and per share amounts) \n",
      "next sib:  None\n",
      "Tag: The Company may enter into foreign currency forward contracts to reduce the Company’s exposure to foreign currency exchange rate fluctuations in the value of foreign currencies. In a foreign currency forward contract, the Company agrees to receive or deliver a fixed quantity of one currency for another, at a pre-determined price at a future date. Forward foreign currency contracts are marked-to-market at the applicable forward rate. Unrealized appreciation (depreciation) on foreign currency forward contracts are recorded on the Consolidated Statements of Assets and Liabilities by counterparty on a net basis, not taking into account collateral posted which is recorded separately, if applicable. Notional amounts of foreign currency forward contract assets and liabilities are presented separately on the Consolidated Schedules of Investments. Purchases and settlements of foreign currency forward contracts having the same settlement date and counterparty are generally settled net and any realized gains or losses are recognized on the settlement date. \n",
      "Next_date: None\n",
      "Next next:  The Company does not utilize hedge accounting and as such, the Company recognizes its derivatives at fair value with changes in the net unrealized appreciation (depreciation) on foreign currency forward contracts recorded on the Consolidated Statements of Operations. \n",
      "next sib:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/yrkcqqr56t955_zz9p9f4swc0000gn/T/ipykernel_4547/2348674701.py:4: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for tag in content.find_all(text=consolidated_schedule_regex):\n"
     ]
    }
   ],
   "source": [
    "consolidated_schedule_regex = re.compile(\n",
    "    r'(?i)^\\s*.*\\s*SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "for tag in content.find_all(text=consolidated_schedule_regex):\n",
    "    print(\"Tag:\", tag.text)\n",
    "    # print(\"Find_next:\", tag.find_next())\n",
    "    print(\"Next_date:\", re.search(date_regex_pattern, tag.text))\n",
    "    # print(\"next:\", tag.next)\n",
    "    print(\"Next next: \", tag.find_next().text)\n",
    "    print(\"next sib: \", tag.find_next_sibling())\n",
    "    # print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
