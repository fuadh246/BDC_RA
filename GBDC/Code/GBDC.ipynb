{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html5lib\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_and_trim(content, content_type):\n",
    "    if content_type == 'HTML':\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    else:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        try:\n",
    "            tag.attrs = None\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    for linebreak in soup.find_all('br'):\n",
    "        linebreak.extract()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Form type</th>\n",
       "      <th>Form description</th>\n",
       "      <th>Filing date</th>\n",
       "      <th>Reporting date</th>\n",
       "      <th>Act</th>\n",
       "      <th>Film number</th>\n",
       "      <th>Accession number</th>\n",
       "      <th>Filings URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>May 03, 2013</td>\n",
       "      <td>March 31, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>13810352</td>\n",
       "      <td>0001144204-13-026113</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>August 08, 2013</td>\n",
       "      <td>June 30, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>131020072</td>\n",
       "      <td>0001144204-13-043799</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/147676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-K</td>\n",
       "      <td>Annual report [Section 13 and 15(d), not S-K I...</td>\n",
       "      <td>December 03, 2013</td>\n",
       "      <td>September 30, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>131254591</td>\n",
       "      <td>0001144204-13-065322</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>February 06, 2014</td>\n",
       "      <td>December 31, 2013</td>\n",
       "      <td>34</td>\n",
       "      <td>14578102</td>\n",
       "      <td>0001144204-14-006255</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>Quarterly report [Sections 13 or 15(d)]</td>\n",
       "      <td>May 08, 2014</td>\n",
       "      <td>March 31, 2014</td>\n",
       "      <td>34</td>\n",
       "      <td>14823110</td>\n",
       "      <td>0001144204-14-028416</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000147...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Form type                                   Form description  \\\n",
       "0      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "1      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "2      10-K  Annual report [Section 13 and 15(d), not S-K I...   \n",
       "3      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "4      10-Q            Quarterly report [Sections 13 or 15(d)]   \n",
       "\n",
       "         Filing date      Reporting date  Act  Film number  \\\n",
       "0       May 03, 2013      March 31, 2013   34     13810352   \n",
       "1    August 08, 2013       June 30, 2013   34    131020072   \n",
       "2  December 03, 2013  September 30, 2013   34    131254591   \n",
       "3  February 06, 2014   December 31, 2013   34     14578102   \n",
       "4       May 08, 2014      March 31, 2014   34     14823110   \n",
       "\n",
       "       Accession number                                        Filings URL  \n",
       "0  0001144204-13-026113  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "1  0001144204-13-043799  https://www.sec.gov/Archives/edgar/data/147676...  \n",
       "2  0001144204-13-065322  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "3  0001144204-14-006255  https://www.sec.gov/Archives/edgar/data/000147...  \n",
       "4  0001144204-14-028416  https://www.sec.gov/Archives/edgar/data/000147...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'GOLUB CAPITAL BDC, Inc.'\n",
    "}\n",
    "filing_links = pd.read_excel(\n",
    "    \"../GBDC__sec_filing_links.xlsx\")\n",
    "filing_links.head(5)\n",
    "date_columns = ['Filing date', 'Reporting date']\n",
    "for col in date_columns:\n",
    "    filing_links[col] = pd.to_datetime(filing_links[col], format='%Y-%m-%d')\n",
    "for col in date_columns:\n",
    "    filing_links[col] = filing_links[col].dt.strftime(\"%B %d, %Y\")\n",
    "filing_links.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is the OG one\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "\n",
    "    return master_table\n",
    "\n",
    "\n",
    "path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/GBDC_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "    print(html_link, qtr_date)\n",
    "    response = requests.get(html_link, headers=headers)\n",
    "    content = parse_and_trim(response.content, 'HTML')\n",
    "    master_table = extract_tables(content, qtr_date)\n",
    "    master_table.to_excel(\n",
    "        writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "    writer.book.save(path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_links = filing_links[filing_links['Reporting date']\n",
    "                            != 'September 30, 2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), keep_default_na=False, skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '-')) if type(x) == str else x)\n",
    "                # new_table = new_table.replace(\n",
    "                #     r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                # new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "# This does the job\n",
    "path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/Test_GBDC_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "    print(html_link, qtr_date)\n",
    "    response = requests.get(html_link, headers=headers)\n",
    "    content = parse_and_trim(response.content, 'HTML')\n",
    "    master_table = extract_tables(content, qtr_date)\n",
    "    master_table.to_excel(\n",
    "        writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "    writer.book .save(path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    consolidated_schedule_regex = re.compile(\n",
    "        r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern1 = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern1, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern1, tag.next.text)\n",
    "        print(date_str)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                table = tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    # Extract the table data into a data frame\n",
    "                    table_data = []\n",
    "                    for row in table.find_all('tr'):\n",
    "                        # Include header cells ('th') if necessary\n",
    "                        columns = row.find_all(['th', 'td'])\n",
    "                        row_data = [column.get_text(strip=True)\n",
    "                                    for column in columns]\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    # Create a data frame from the table data and add it to the list\n",
    "                    table_df = pd.DataFrame(table_data)\n",
    "                    new_table = table_df.applymap(lambda x: unicodedata.normalize(\n",
    "                        'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '0').replace('%', '')) if type(x) == str else x)\n",
    "\n",
    "                    new_table = new_table.replace(\n",
    "                        r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                    table_df = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                    if len(table_df.columns) > 10:\n",
    "                        data_frames.append(table_df)\n",
    "\n",
    "                        if master_table is None:\n",
    "                            master_table = table_df\n",
    "                        else:\n",
    "                            master_table = pd.concat(\n",
    "                                [master_table, table_df], ignore_index=True)\n",
    "\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), keep_default_na=False, skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '0').replace('%', '')) if type(x) == str else x)\n",
    "\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table_fun(soi_table_df, process_tables_shapes):\n",
    "    print(1, 'shape:', soi_table_df.shape)\n",
    "    soi_table_df = soi_table_df.replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "    soi_table_df = soi_table_df.replace(r'\\n', '', regex=True)\n",
    "    print(2, 'shape:', soi_table_df.shape)\n",
    "    print(6, 'shape:', soi_table_df.shape)\n",
    "    soi_table_df = soi_table_df.replace('â€”', 0)\n",
    "    soi_table_df = soi_table_df.replace('-', 0)\n",
    "    print(7, 'shape:', soi_table_df.shape)\n",
    "    soi_table_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    soi_table_df = soi_table_df.applymap(\n",
    "        lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    pattern = r'Total\\s+Investments'\n",
    "    # Use the apply function to check if the pattern is in any column for each row\n",
    "    matching_rows = soi_table_df.apply(\n",
    "        lambda row: row.str.contains(pattern, flags=re.IGNORECASE, regex=True).any(), axis=1)\n",
    "    # Find the index of the first row that matches the pattern\n",
    "    # Slice the DataFrame to keep only the rows up to and including the first matching row\n",
    "    if soi_table_df[matching_rows].index[0] < 20:\n",
    "        soi_table_df = soi_table_df.loc[:soi_table_df[matching_rows].index[1]].reset_index(\n",
    "            drop=True)\n",
    "    else:\n",
    "        soi_table_df = soi_table_df.loc[:soi_table_df[matching_rows].index[0]].reset_index(\n",
    "            drop=True)\n",
    "\n",
    "# removest end extra\n",
    "    pattern = r'Net asset value per common share|How We Addressed the Matter in Our Audit'\n",
    "    matching_rows = soi_table_df.apply(\n",
    "        lambda row: row.str.contains(pattern, flags=re.IGNORECASE, regex=True).any(), axis=1)\n",
    "\n",
    "    # Check if the pattern exists in the DataFrame\n",
    "    if matching_rows.any():\n",
    "        # Extract rows from the first occurrence onwards\n",
    "        soi_table_df = soi_table_df.iloc[matching_rows.idxmax(\n",
    "        )+1:].reset_index(drop=True)\n",
    "\n",
    "    # removing all col name\n",
    "    pattern = r'(?:Spread\\s*Above|cost|Percentage|Above)'\n",
    "    matching_rows = soi_table_df.apply(\n",
    "        lambda row: row.str.contains(pattern, flags=re.IGNORECASE, regex=True).any(), axis=1)\n",
    "    soi_table_df = soi_table_df[~matching_rows]\n",
    "\n",
    "    print(0, 'shape:', soi_table_df.shape)\n",
    "    soi_table_df = soi_table_df.dropna(how='all', axis=1).reset_index(\n",
    "        drop=True)\n",
    "    print(3, 'shape:', soi_table_df.shape)\n",
    "    soi_table_df = soi_table_df.dropna(how='all', axis=0).reset_index(\n",
    "        drop=True)\n",
    "    print(4, 'shape:', soi_table_df.shape)\n",
    "    soi_table_df.dropna().reset_index(\n",
    "        drop=True)\n",
    "    print(5, 'shape:', soi_table_df.shape)\n",
    "\n",
    "\n",
    "# drops the sub total\n",
    "    soi_table_df = soi_table_df.dropna(subset=[soi_table_df.columns[0]])\n",
    "\n",
    "    num_cols = soi_table_df.shape[1]\n",
    "    data_col_mapper = dict(zip(soi_table_df.columns.to_list(), [\n",
    "        i for i in range(0, num_cols)]))\n",
    "    soi_table_df = soi_table_df.rename(columns=data_col_mapper)\n",
    "\n",
    "    for index, row in soi_table_df.iterrows():\n",
    "        for column in soi_table_df.columns:\n",
    "            if pd.isna(row[column]):\n",
    "                # Find the next column in the same row\n",
    "                next_column = soi_table_df.columns.get_loc(column) + 1\n",
    "                if next_column < len(soi_table_df.columns):\n",
    "                    next_column_name = soi_table_df.columns[next_column]\n",
    "                    # Replace NaN value with the value from the next column\n",
    "                    soi_table_df.at[index, column] = row[next_column_name]\n",
    "                    # Set the next column to NaN\n",
    "                    soi_table_df.at[index, next_column_name] = np.nan\n",
    "\n",
    "    for index, row in soi_table_df.iterrows():\n",
    "        for column in soi_table_df.columns:\n",
    "            if pd.isna(row[column]):\n",
    "                while True:\n",
    "                    # Find the next column in the same row\n",
    "                    next_column = soi_table_df.columns.get_loc(column) + 1\n",
    "                    if next_column < len(soi_table_df.columns):\n",
    "                        next_column_name = soi_table_df.columns[next_column]\n",
    "                        # Replace NaN value with the value from the next column\n",
    "                        soi_table_df.at[index, column] = row[next_column_name]\n",
    "                        # Set the next column to NaN\n",
    "                        soi_table_df.at[index, next_column_name] = np.nan\n",
    "                        column = next_column_name\n",
    "                    else:\n",
    "                        # No more columns to replace, break out of the loop\n",
    "                        break\n",
    "\n",
    "    soi_table_df = soi_table_df.dropna(axis=1, thresh=10)\n",
    "    soi_table_df = soi_table_df.dropna(how='all', axis=1).reset_index(\n",
    "        drop=True)\n",
    "\n",
    "    process_tables_shapes.append(soi_table_df.shape)\n",
    "    print(soi_table_df.info())\n",
    "\n",
    "    return soi_table_df\n",
    "\n",
    "\n",
    "process_tables = {}\n",
    "process_tables_shape = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does the job\n",
    "path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/Tr_GBDC_Investment.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "    print(html_link, qtr_date)\n",
    "    response = requests.get(html_link, headers=headers)\n",
    "    content = parse_and_trim(response.content, 'HTML')\n",
    "    master_table = extract_tables(content, qtr_date)\n",
    "    process_table = process_table_fun(master_table, process_tables_shape)\n",
    "    process_table.to_excel(\n",
    "        writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "    writer.book .save(path)\n",
    "writer.close()\n",
    "\n",
    "\n",
    "# last work Nov 6th 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_tables_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), keep_default_na=False, skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('-', '0').replace('%', '')) if type(x) == str else x)\n",
    "\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "# date = \"December 31, 2013\"\n",
    "# url = filing_links[filing_links['Reporting date']\n",
    "#                    == date]['Filings URL'].values[0]\n",
    "# # webbrowser.open(url=url)\n",
    "# response = requests.get(url=url, headers=headers)\n",
    "# content = parse_and_trim(response.content, \"HTML\")\n",
    "# master_table = extract_tables(content, date)\n",
    "# contentDUP = content\n",
    "# master_tableDUP = master_table\n",
    "\n",
    "\n",
    "# This does the job\n",
    "# path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/Master_tables_GBDC_Investment.xlsx'\n",
    "# if not os.path.exists('../MT_csv_file'):\n",
    "#     os.makedirs('../MT_csv_file')\n",
    "\n",
    "# writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content, qtr_date)\n",
    "#     master_table.to_excel(\n",
    "#         writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "#     master_table.to_csv(\n",
    "#         '../MT_csv_file/'+qtr_date.replace(',', '')+'.csv')\n",
    "#     writer.book .save(path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 31 2015\n",
      "December 31, 2015 december312015 december312015\n",
      "September 30, 2015 december312015 september302015\n",
      "(63, 27)\n"
     ]
    }
   ],
   "source": [
    "html_link = \"https://www.sec.gov/Archives/edgar/data/1476765/000114420416079295/v430459_10q.htm\"\n",
    "qtr_date = \"December 31 2015\"\n",
    "response = requests.get(html_link, headers=headers)\n",
    "content = parse_and_trim(response.content, 'HTML')\n",
    "master_table = extract_tables(content, qtr_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 31, 2015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "December 31, 2015 december312015 december312015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "September 30, 2015 december312015 september302015\n",
      "(505, 27)\n"
     ]
    }
   ],
   "source": [
    "data_frames = []\n",
    "data_frames_shapes = []\n",
    "\n",
    "\n",
    "def extract_tables(soup_content, qtr_date):\n",
    "    master_table = None\n",
    "    print(qtr_date)\n",
    "    if qtr_date == 'December 31, 2015' or qtr_date == 'June 30, 2016':\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*(\\(.*\\)|)\\s*-.*\\s*\\(.*\\)$')\n",
    "    else:\n",
    "        consolidated_schedule_regex = re.compile(\n",
    "            r'(?i)^\\s*.*\\s*CONSOLIDATED\\s+SCHEDULE(S|)\\s+OF\\s+INVESTMENTS\\s*.*\\s*$')\n",
    "    date_regex_pattern = r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    # date_regex_pattern2 = r'\\bAs\\s+of\\s+([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})\\b'\n",
    "    for tag in soup_content.find_all(text=re.compile(consolidated_schedule_regex)):\n",
    "        date_str = re.search(date_regex_pattern, tag.find_next().text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern, tag.next.text)\n",
    "        if date_str is None:\n",
    "            date_str = re.search(date_regex_pattern,\n",
    "                                 tag.find_next().next.next.next.text)\n",
    "        if date_str is not None:\n",
    "            date_str = str(date_str.group(1))\n",
    "            date_str = unicodedata.normalize('NFKD', date_str)\n",
    "            qtr_date_cleaned = qtr_date.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            date_str_cleaned = date_str.replace(',', '').replace(\n",
    "                ' ', '').replace('\\n', '').lower()\n",
    "            print(date_str, qtr_date_cleaned, date_str_cleaned)\n",
    "\n",
    "            if qtr_date_cleaned == date_str_cleaned:\n",
    "                html_table = tag.find_next('table')\n",
    "                new_table = pd.read_html(\n",
    "                    html_table.prettify(), keep_default_na=False, skiprows=0, flavor='bs4')[0]\n",
    "                new_table = new_table.applymap(lambda x: unicodedata.normalize(\n",
    "                    'NFKD', x.strip().strip(u'\\u200b').replace('â€”', '0').replace('%', '')) if type(x) == str else x)\n",
    "\n",
    "                new_table = new_table.replace(\n",
    "                    r'^\\s*$', np.nan, regex=True).replace(r'^\\s*\\$\\s*$', np.nan, regex=True)\n",
    "                new_table = new_table.dropna(how='all', axis=0)\n",
    "\n",
    "                if master_table is None:\n",
    "                    master_table = new_table\n",
    "                else:\n",
    "                    master_table = pd.concat(\n",
    "                        [master_table, new_table], ignore_index=True)\n",
    "            # print(master_table)\n",
    "\n",
    "            master_table = master_table.replace('N/A', 'No Value')\n",
    "\n",
    "    master_table = master_table.applymap(\n",
    "        lambda x: x.strip().strip(u'\\u200b') if type(x) == str else x)\n",
    "    master_table = master_table.replace(r'^\\s*$', np.nan, regex=True).replace(\n",
    "        r'^\\s*\\$\\s*$', np.nan, regex=True).replace(r'^\\s*\\)\\s*$', np.nan, regex=True)\n",
    "    print(master_table.shape)\n",
    "    data_frames.append(master_table)\n",
    "    data_frames_shapes.append(master_table.shape)\n",
    "    return master_table\n",
    "\n",
    "\n",
    "date = \"December 31, 2015\"\n",
    "url = filing_links[filing_links['Reporting date']\n",
    "                   == date]['Filings URL'].values[0]\n",
    "# webbrowser.open(url=url)\n",
    "response = requests.get(url=url, headers=headers)\n",
    "content = parse_and_trim(response.content, \"HTML\")\n",
    "master_table = extract_tables(content, date)\n",
    "contentDUP = content\n",
    "master_tableDUP = master_table\n",
    "master_table.to_csv('test.csv')\n",
    "\n",
    "filing_links = filing_links[filing_links['Reporting date']\n",
    "                            != 'September 30, 2017']\n",
    "\n",
    "# This does the job\n",
    "# path = '/Users/fuadhassan/Desktop/BDC_RA/GBDC/Master_tables_GBDC_Investment.xlsx'\n",
    "# if not os.path.exists('../MT_csv_file'):\n",
    "#     os.makedirs('../MT_csv_file')\n",
    "\n",
    "# writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "# for qtr_date, html_link in zip(filing_links['Reporting date'], filing_links['Filings URL']):\n",
    "#     print(html_link, qtr_date)\n",
    "#     response = requests.get(html_link, headers=headers)\n",
    "#     content = parse_and_trim(response.content, 'HTML')\n",
    "#     master_table = extract_tables(content, qtr_date)\n",
    "#     master_table.to_excel(\n",
    "#         writer, sheet_name=qtr_date.replace(',', ''), index=False)\n",
    "#     master_table.to_csv(\n",
    "#         '../MT_csv_file/'+qtr_date.replace(',', '')+'.csv')\n",
    "#     writer.book .save(path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
